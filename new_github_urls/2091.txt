skip to content you signed in with another tab or window. reload to refresh your session. you signed out in another tab or window. reload to refresh your session. you switched accounts on another tab or window. reload to refresh your session. dismiss alert ros2 / rclcpp public notifications you must be signed in to change notification settings fork 411 star 528 new issue have a question  this project?  for a free github account to open an issue and  its maintainers and the community.  for github by clicking  for github, you agree to our  of service and  statement . well occasionally send you account related s. already on github?  to your account jump to bottom add tracing instrumentation for intra-process #2091 merged clalancette merged 1 commit into ros2 : rolling from ymski : add-intra-process-tracepoint apr 13, 2023 merged add tracing instrumentation for intra-process #2091 clalancette merged 1 commit into ros2 : rolling from ymski : add-intra-process-tracepoint apr 13, 2023 conversation copy link contributor ymski commented jan 25, 2023 overview hi, i'm a member of caret development team. we would like to officially support some tracepoints added by caret in the rclcpp fork and ld_preload. the current tracetools do not support intra-process communication. this pull request adds tracing instrumentation to calculate the processing time from publish to  callback . these trace points are important in performance analysis. processing that may become a bottleneck, such as strict time constraints or handling large data, often uses intra-process communication. sorry, something went wrong. all reactions copy link contributor author ymski commented jan 25, 2023 tracepoints we propose to add the following tracepoints mainly targeting ring buffers. tracepoint details: rclcpp_intra_publish tracepoint for intra-process communication. unlike inter-process communication, intra-process communication may result in data copying. for this reason, rclcpp_intra_publish is defined separately from rclcpp_publish , which has similar arguments. args publisher_handle: publisher handle address. message: message address. construct_ring_buffer tracepoint for construction. args buffer: buffer address. capacity: buffer capacity. ring_buffer_enqueue tracepoint for enqueuing. used for binding of published and subscribed messages by index. args buffer: buffer address. index: index to which messages are written overwriting_occurred: indicates whether lost has occurred. ring_buffer_dequeue tracepoint for dequeuing. used for binding of published and subscribed messages by index. args buffer: buffer address. index: index to which messages are read. ring_buffer_clear not used, but only api is provided args buffer: buffer address. all reactions sorry, something went wrong. copy link contributor author ymski commented jan 25, 2023 sequence diagram the following may be helpful as a sequence of processing related to the trace points we have added. enqueue: dequeue: dispatch: all reactions sorry, something went wrong. copy link contributor author ymski commented jan 25, 2023 the relationship between each tracepoint is summarized in the following er diagram. we are now able to track messages communicated using intra-process as below. the above method of tracking messages using thread id can be used based on the following condition the thread id does not change during publish, , etc. we are concerned that changes in specifications due to future ros2 development may make the above methods unavailable. all reactions sorry, something went wrong. ymski mentioned this pull request jan 25, 2023 add intra process tracepoint ros2/ros2_tracing#30 merged copy link contributor author ymski commented jan 25, 2023 notes since this pr has multiple repositories in scope, it is necessary to make changes to multiple repositories at the same time. see also: add intra process tracepoint ros2_tracing#30 all reactions sorry, something went wrong. copy link collaborator fujitatomoya commented jan 25, 2023 @christophebedard the current tracetools do not support intra-process communication. is this intention? i do not really know the history, but adding intra-process communication tracepoint could be useful? @ymski once it is ready to review, we are happy to review. and probably it would be nice to talk  this client wg as well. all reactions sorry, something went wrong. copy link member christophebedard commented jan 25, 2023 happy to see this pr @ymski ! the current tracetools do not support intra-process communication. is this intention? i do not really know the history, but adding intra-process communication tracepoint could be useful? @fujitatomoya we indeed never instrumented intra-process communications in a way that allows tracking messages from publication to callback, because supporting the default use-case (i.e., network) was enough. if you search for "intra" in the ros2_tracing design document, you'll see that intra-process s/callbacks are supported/instrumented, but that's not enough to track messages from publication to callback: https://github.com/ros2/ros2_tracing/blob/rolling/doc/design_ros_2.md . this is definitely useful.  1 fujitatomoya reacted with thumbs up emoji all reactions  1 reaction sorry, something went wrong. copy link member christophebedard commented jan 25, 2023  edited loading the above method of tracking messages using thread id can be used based on the following condition the thread id does not change during publish, , etc. we are concerned that changes in specifications due to future ros2 development may make the above methods unavailable. i understand that this works currently, but yeah it might stop working in the future. instead of relying on the tid to link an rclcpp_intra_publish event to its corresponding ring_buffer_enqueue event and link a ring_buffer_dequeue event to its corresponding callback_start , is there a way to link the publisher handle to the ring buffer address and link the  handle to the ring buffer address? i've taken a quick look at the intra-process code, and this seems a bit hard to do due to all the layers ( ringbufferimplementation / typedintraprocessbuffer ) and the fact that the ring buffer seems to be owned by the . it might require a new tracepoint before/after construct_ring_buffer . this would allow linking publisher handle/message -> buffer address -> index -> callback, which would be more robust and would probably not break in the future. note that tracepoints are part of the tracetools abi (even if it's not a public api/abi like rclcpp is), so having to change the tracepoints (e.g., removing some of them or modifying their arguments) in the future would lead to a bit of a headache. all reactions sorry, something went wrong. copy link contributor author ymski commented jan 26, 2023  for your reply. the draft pr was used to confirm the error in the github actions, but it does not seem to be resolved without merging the relevant pr. i have now changed from draft pr to pr. i think we should continue this discussion on the issues mentioned above.  1 christophebedard reacted with thumbs up emoji all reactions  1 reaction sorry, something went wrong. ymski marked this pull request as ready for review january 26, 2023 02:37 ymski requested review from ivanpauno , hidmic and wjwwood as code owners january 26, 2023 02:37 copy link contributor author ymski commented jan 26, 2023  edited loading  for your comment. @fujitatomoya probably it would be nice to talk  this client wg as well. i am glad you said this. but i am sorry, i am not a good english speaker, so it would be helpful if we could talk on a text.  for your comment. @christophebedard as we were concerned, the method of using tid may not work in the future. we will reconsider the trace points, including linking publisher_handle to buffer_address. incidentally, i believe you are considering linking using message addresses. to do this, i think we need to add a tracepoint for each copy of the message. since we have copies everywhere in rclcpp , we are concerned that adding tracepoints for all of them would have a large scope. for reference, here is what we did with caret to add tracepoints to the copies. message_construct is the tracepoint for copy. tier4@ f771072 all reactions sorry, something went wrong. copy link collaborator fujitatomoya commented jan 26, 2023 @ymski thanks for iterating, i am happy to do review on this  all reactions sorry, something went wrong. fujitatomoya reviewed jan 26, 2023 view reviewed changes rclcpp/include/rclcpp/experimental/buffers/ring_buffer_implementation.hpp outdated show resolved hide resolved rclcpp/include/rclcpp/experimental/buffers/ring_buffer_implementation.hpp outdated show resolved hide resolved fujitatomoya approved these changes jan 27, 2023 view reviewed changes copy link collaborator fujitatomoya commented jan 27, 2023 waiting for another review, and after that i will start ci. all reactions sorry, something went wrong. copy link contributor author ymski commented jan 27, 2023 waiting for another review, and after that i will start ci. @fujitatomoya  for approving. however, after discussion with @christophebedard , it looks like the trace points will need to be changed. sorry, but it would be helpful if you could wait a bit longer to start the ci. all reactions sorry, something went wrong. copy link contributor author ymski commented jan 27, 2023 @christophebedard i understand that for robustness reasons it is preferable to link by address.i have been thinking  how to link tracepoints using message addresses. it would be possible to link everything from rclpp_intra_publish to ring_buffer_dequeue by address if a tracepoint ( construcrt_message((original_message_address, new_message_address) )is added for each copy of the message. by changing the index argument of the trace point of ring_buffer to message address, the following link can be created. rclcpp_intra_publish -(original_message_address)-> copy_message -(new_message_address)-> ring_buffer_enqueue the pubsiher_handle and buffer links may no longer be necessary if tracepoints are added for copies of messages. i think the link on the enqueue side is now ok, but the link without tid on the dequeue side is still unresolved. i think this is the same problem as the way rclcpp_take and callback_start are linked in ros2_tracing, but i couldn't find in the documentation how this is achieved. how does ros2_tracing solve this? i think if there is a trace point like rclcpp_dipatch_ (&callback, &message) during dispatch , it would be possible to link them by address. all reactions sorry, something went wrong. copy link contributor author ymski commented feb 9, 2023 hi, @christophebedard ! i thought i was missing an explanation of the method proposed above, so i will capture it. suppose we have a link between publisher_handle and buffer_address. however, this assumption alone may not work when publishing is done in asynchronous multi-threaded mode, as in the example below. the following is an extract from the trace data, showing only the trace points of rclcpp_intra_pub and callback_start , the events for which the processing time is to be measured. here we consider matching rclcpp_intra_publish and callabck_start . rclcpp_intra_publish(@publisher_handler1, @msg1) #1 rclcpp_intra_publish(@publisher_handler2, @msg2) #2 callback_start(@callback) #??? callback_start(@callback) #??? in this case, the matching between rclcpp_intra_publish and callback_start is not determined as shown in case1, case2. case1 rclcpp_intra_publish(@publisher_handler1, @msg1) #1 rclcpp_intra_publish(@publisher_handler2, @msg2) #2 enqueue(msg1) enqueue(msg2) dequeue(msg1) callback_start(@callback) #1 dequeue(msg2) callback_start(@callback) #2 case2 rclcpp_intra_publish(@publisher_handler1, @msg1) #1 rclcpp_intra_publish(@publisher_handler2, @msg2) #2 enqueue(msg2) dequeue(msg2) callback_start(@callback) #2 enqueue(msg1) dequeue(msg1) callback_start(@callback) #1 the same problem can occur when copying messages. this is due to the lack of information  where the messages used to execute the callback came from. therefore, the method described above is proposed to trace the messages. case1 with message track: rclcpp_intra_publish(@publisher_handler1, @msg1) #1 rclcpp_intra_publish(@publisher_handler2, @msg2) #2 enqueue(msg1) enqueue(msg2) dequeue(msg1) dispatch_callback(@msg1, @callback) callback_start(@callback) #1 dequeue(msg2) dispatch_callback(@msg2, @callback) callback_start(@callback) #2 (note: it might be more ideal if callback_start had a message address) what do you think of the robustness of this approach? all reactions sorry, something went wrong. copy link member christophebedard commented feb 10, 2023 @ymski  for the example. just to take a step back: on the publisher side, i think we might be able to assume that rclcpp_intra_publish and ring_buffer_enqueue always happen on the same thread. therefore, we could use the tid to link the events. we would then avoid needing the "message copy" tracepoint. however, if there are n intraprocess s with the same topic, i think we'll get n ring_buffer_enqueue events after a single rclcpp_intra_publish event, correct? then we just have to make sure we can link the rclcpp_intra_publish event to all n ring_buffer_enqueue events (e.g., all consecutive ring_buffer_enqueue events after a rclcpp_intra_publish event on the same thread). on the  side, we shouldn't assume that ring_buffer_dequeue and callback_start happen on the same thread. then i think your solution makes sense. @wjwwood what do you think? all reactions sorry, something went wrong. copy link member christophebedard commented feb 10, 2023 i think this is the same problem as the way rclcpp_take and callback_start are linked in ros2_tracing, but i couldn't find in the documentation how this is achieved. how does ros2_tracing solve this? rclcpp_take is linked to the rcl_take and rmw_take events by expecting the events to happen in the following order with the same message value: rmw_take , rcl_take , and rclcpp_take . note that rcl_take isn't really useful here, and we don't need to assume that the events happen on the same thread. rclcpp_take 's rmw__handle field is linked to the  handle ( rcl ) using information collected during initialization: rcl__init contains both the  handle and the rmw__handle . then the callback field of the callback_start / callback_end tracepoints is mapped to the  handle using information collected during initialization: rclcpp__callback_added contains both the  handle and the callback . so the callback_start / callback_end events can be linked to the *_take events and the message and source_timestamp values. and, as you probably know, this source_timestamp value then allows us to link this message to the original message on the publisher's side. all reactions sorry, something went wrong. copy link contributor author ymski commented feb 10, 2023  edited loading @christophebedard thanks for sharing how to link in ros2 tracing. i also understand the method you suggested. however, if there are n intraprocess s with the same topic, i think we'll get n ring_buffer_enqueue events after a single rclcpp_intra_publish event, correct? i think the same. here is an extract of the trace data when the  of the cyclic_pipeline demo is simply increased to two. they have the same tid and can be linked. ( note that there are differences in variable names and additional trace points) [18:04:10.875980976] (+0.000000866) docker ros2:callback_start: { cpu_id = 3 }, { vpid = 12595, vtid = 12595, procname = "cyclic_pipeline" }, { callback = 0x556c47321a20, is_intra_process = 1 } [18:04:11.876125164] (+1.000144188) docker ros2:rclcpp_intra_publish: { cpu_id = 3 }, { vpid = 12595, vtid = 12595, procname = "cyclic_pipeline" }, { publisher_handle = 0x556c472e8510, message = 0x556c47470620 } [18:04:11.876133342] (+0.000008178) docker ros2:message_construct: { cpu_id = 3 }, { vpid = 12595, vtid = 12595, procname = "cyclic_pipeline" }, { original_message = 0x556c47470620, constructed_message = 0x556c47470660 } [18:04:11.876136013] (+0.000002671) docker ros2:ring_buffer_enqueue: { cpu_id = 3 }, { vpid = 12595, vtid = 12595, procname = "cyclic_pipeline" }, { buffer = 0x556c47458100, index = 4, is_full = 0 } [18:04:11.876139450] (+0.000003437) docker ros2:ring_buffer_enqueue: { cpu_id = 3 }, { vpid = 12595, vtid = 12595, procname = "cyclic_pipeline" }, { buffer = 0x556c4746cdc0, index = 4, is_full = 0 } [18:04:11.876142329] (+0.000002879) docker ros2:callback_end: { cpu_id = 3 }, { vpid = 12595, vtid = 12595, procname = "cyclic_pipeline" }, { callback = 0x556c47321a20 } i understand the linking method of the publisher's side as follows. the linking method by tid can be used to obtain a list of enqueued addresses. on the other hand, the linking method using the publisher_handle and buffer address can obtain a list of addresses that may be enqueued. (although this can be a bit of a pain if s are added later). this requires using the information at initialization (e.g. rcl_publisher_init , rcl__init , rclcpp_buffer_init (exactly what we want right now.)). is this understanding correct? all reactions sorry, something went wrong. copy link member christophebedard commented feb 14, 2023 i'm not sure i understand what the constructed_message value in the ros2:message_construct tracepoint is for. could you share the complete trace, or a portion of the trace that contains all relevant tracepoints (i.e., all init tracepoints, all publisher and  tracepoints)? all reactions sorry, something went wrong. copy link contributor author ymski commented feb 14, 2023 i am sorry @christophebedard i thought i had to respond quickly and gave you data with unnecessary trace point logs. sorry for the confusion. (construct_message was an experimental trace point i added to track a copy of the message. it is not yet included in this pr). i thought we wanted to make sure the tid matched for successive enqueues. in that case, the following repository, which contains examples of measured applications and trace data, would be helpful. https://github.com/ymski/intra_process_demo by the way, i am not going to stick to the tid linking method. my concern was the magnitude of the impact of adding trace points to track copies, and i think a linking method that does not use tid is a healthier and preferable implementation for the ros2 project.  1 christophebedard reacted with thumbs up emoji all reactions  1 reaction sorry, something went wrong. copy link member christophebedard commented feb 14, 2023 , that repository and example traces are helpful. let me know once you implement the solution to avoid linking using tids for both pub and sub. also, how do you plan on linking the ring buffer to its , i.e., rcl__init to construct_ring_buffer ? do you simply assume that the construct_ring_buffer event always comes after the rcl__init event on the same thread? example from the 1pub-2sub-two-thread trace in your repository: ros2:rcl__init: { cpu_id = 10 }, { vpid = 3862, procname = "cyclic_pipeline", vtid = 3862 }, { _handle = 0x5582898f8160, node_handle = 0x558289624560, rmw__handle = 0x5582899099a0, topic_name = "/topic1", queue_depth = 10 } ros2:construct_ring_buffer: { cpu_id = 10 }, { vpid = 3862, procname = "cyclic_pipeline", vtid = 3862 }, { buffer = 0x558289909f00, capacity = 10 } all reactions sorry, something went wrong. copy link contributor author ymski commented feb 15, 2023 do you simply assume that the construct_ring_buffer event always comes after the rcl__init event on the same thread? no. it would not be a good idea to make the above assumption if we do not use tids for links. as you know, the tracepoints added in the current pr are not enough to link s and buffers. therefore, i am considering adding a tracepoint called rclcpp_buffer_init(topic_name, buffer) to _intra_process_buffer.hpp . the relationship between each trace point is shown in the diagram below. red node is the run-phase tracepoint. yellow node is the init-phase tracepoint. solid lines are combinations of nodes that can be directly linked. dotted lines represent combinations that can be linked via tracepoints during initialization. it is the relationship between trace points that were linked using tid. all reactions sorry, something went wrong. copy link member christophebedard commented feb 16, 2023  edited loading therefore, i am considering adding a tracepoint called rclcpp_buffer_init(topic_name, buffer) to _intra_process_buffer.hpp . i'm guessing your diagram should link rclcpp_buffer_init to rclcpp__init / rcl__init instead of rcl_publisher_init , since the buffer belongs to the ? also, what if you have 2  with the same topic name? you wouldn't be able to know which buffer corresponds to which . or do you assume that a publisher with a given topic name will always put the message in the ring buffers of s with the same topic name? all reactions sorry, something went wrong. copy link contributor author ymski commented feb 16, 2023 thanks fou your comment @christophebedard the above post was a bad idea.  forget  it. (there are a few things wrong with the diagram too) i will rethink it, including your advice. this may take some time.  1 christophebedard reacted with thumbs up emoji all reactions  1 reaction sorry, something went wrong. copy link contributor author ymski commented feb 22, 2023 @christophebedard sorry for the delay in responding. i have considered how to link them without using tid. the conclusion is that on the dequeue side you can link without using a tid, but on the enqueue side it appears that we need to use a tid. the details are as follows. link publish and enqueue unfortunately, it does not seem to be possible to add a trace point to find out which publisher enqueued to which buffer. it is preferable to use the tid method here. link dequeue and callback_start it appears that s and buffers can be linked as follows. a test implementation can be found below. 55cbbd6 ros2/ros2_tracing@ 4f5dda9 this shows the relationship between trace points, including newly added. all reactions sorry, something went wrong. copy link member christophebedard commented feb 23, 2023 @ymski looking at it quickly, that looks good. i'll try generating a trace using your intra_process_demo and look at the output to properly validate. one question: do you link the dispatched message to the callback (buffer dequeue -> callback start/end) by simply taking the next callback start event for the  corresponding to the buffer? or do you want to add another tracepoint to link the dispatched message to the callback start event (as you suggested), or possibly add a new message field to the callback_start tracepoint? all reactions sorry, something went wrong. copy link contributor author ymski commented feb 24, 2023 @christophebedard  for your review. one question: do you link the dispatched message to the callback (buffer dequeue -> callback start/end) by simply taking the next callback start event for the  corresponding to the buffer? or do you want to add another tracepoint to link the dispatched message to the callback start event (as you suggested), or possibly add a new message field to the callback_start tracepoint?  for your question  dispatched message to the callback. i was just  to discuss that as well. i think from a robustness point of view it would be desirable to add trace points using message addresses. as for how to add information  the message address, i think it should be convenient for future development of ros2_tracing. as a maintainer, what do you think is better? all reactions sorry, something went wrong. copy link member christophebedard commented mar 1, 2023 in general, i would prefer adding a message address field to the existing callback_start event instead of creating a new tracepoint just for this. the callback_start tracepoint is used for , service, and timer callbacks, so the message address value could be nullptr for timer callbacks, since there are no messages for timer callbacks. however, i think we don't strictly need it for the moment: it currently works fine in ros 2 (as i explained in #2091 (comment) ) and it should work fine for this (as i mentioned above). since the message address can be re-used anyway (so you could get multiple calback_start events for the same callback and with the same message address value), you must still match the message to the callback using the order of the events, so it doesn't make the process that much more robust. therefore, i would prefer not to do that until we think we really need it.  1 ymski reacted with thumbs up emoji all reactions  1 reaction sorry, something went wrong. copy link contributor author ymski commented mar 10, 2023 @christophebedard thanks for the discussion and confirmation of the trace points. i have implemented what you have summarised in the discussion so far and updated the pr. by the way, we use the buffer index to trace messages, but now that i think  it, it seems more appropriate to use addresses. is it ok to switch to tracepoints using addresses? also, as i mentioned in the discussion above , it would be useful to be able to see how much data has accumulated in the buffer. if there is demand, the number of data could be included in ring_buffer_enqueue and ring_buffer_dequeue . (however, this can still be done with the current trace points by counting the trace data to check the number of pieces of data in the queue. this suggestion is not essential for knowing the number of data in the queue, so this may be an inappropriate suggestion from the point of view of the minimum configuration of tracepoints.) all reactions sorry, something went wrong. copy link member mjcarroll commented mar 15, 2023 by the way, we use the buffer index to trace messages, but now that i think  it, it seems more appropriate to use addresses. is it ok to switch to tracepoints using addresses? i think addresses make sense for consistency. also, as i mentioned in #2091 (comment) , it would be useful to be able to see how much data has accumulated in the buffer. if there is demand, the number of data could be included in ring_buffer_enqueue and ring_buffer_dequeue. in my mind, while the value can be reconstructed, this seems like a relatively cheap thing to trace to avoid having going through that exercise.  1 ymski reacted with thumbs up emoji all reactions  1 reaction sorry, something went wrong. copy link contributor author ymski commented mar 20, 2023 @mjcarroll i am glad to hear that comment. unfortunately though, when i thought  the implementation, it turned out that replacing the index with a message address might be a bit difficult. this is due to the template implementation of the ring buffer. as the results of the analysis are not affected by either index or message address, i will adopt the index. i have also added accumulated data to record the number of data stored internally in the enqueue and dequeue of the ring buffer. if there are no problems with the trace points to be added, we can proceed to code review. what do you think @christophebedard ? all reactions sorry, something went wrong. copy link member christophebedard commented mar 21, 2023 sorry for the delay, i've been pretty busy. i'll take a look tomorrow. unfortunately though, when i thought  the implementation, it turned out that replacing the index with a message address might be a bit difficult. this is due to the template implementation of the ring buffer. makes sense. indexes are fine then. all reactions sorry, something went wrong. christophebedard requested changes mar 22, 2023 view reviewed changes copy link member christophebedard left a comment there was a problem hiding this comment. choose a reason for hiding this comment the reason will be displayed to describe this comment to others. learn more . choose a reason spam abuse off topic outdated duplicate resolved hide comment some of these suggestions correspond to my suggestions in the ros2_tracing pr. sorry again for the delay. sorry, something went wrong.  1 ymski reacted with eyes emoji all reactions  1 reaction rclcpp/include/rclcpp/experimental/buffers/intra_process_buffer.hpp outdated show resolved hide resolved rclcpp/include/rclcpp/experimental/buffers/ring_buffer_implementation.hpp outdated show resolved hide resolved rclcpp/include/rclcpp/experimental/buffers/ring_buffer_implementation.hpp outdated show resolved hide resolved rclcpp/include/rclcpp/experimental/_intra_process_buffer.hpp outdated show resolved hide resolved christophebedard requested changes mar 23, 2023 view reviewed changes rclcpp/include/rclcpp/experimental/buffers/ring_buffer_implementation.hpp outdated show resolved hide resolved cwecht mentioned this pull request mar 24, 2023 intra process tracepoints safe-ros/ros2_profiling#24 merged ymski mentioned this pull request apr 12, 2023 add release note  tracepoints for intra-process communication ros2/ros2_documentation#3449 merged christophebedard approved these changes apr 12, 2023 view reviewed changes copy link member christophebedard left a comment  edited loading there was a problem hiding this comment. choose a reason for hiding this comment the reason will be displayed to describe this comment to others. learn more . choose a reason spam abuse off topic outdated duplicate resolved hide comment this looks good from my side (see ros2/ros2_tracing#30 ). i'll let @fujitatomoya rebase the branch on rolling and review, then i'll run ci. then we can squash and merge this eventually. sorry, something went wrong. all reactions fujitatomoya approved these changes apr 12, 2023 view reviewed changes copy link collaborator fujitatomoya left a comment there was a problem hiding this comment. choose a reason for hiding this comment the reason will be displayed to describe this comment to others. learn more . choose a reason spam abuse off topic outdated duplicate resolved hide comment unfortunately though, when i thought  the implementation, it turned out that replacing the index with a message address might be a bit difficult. this is due to the template implementation of the ring buffer. makes sense. indexes are fine then. i think that in design, this information is better to be cached in the data object (in this particular case, ring buffer), not handlers nor users. and indexes are just fine to use for these cases, i believe. sorry, something went wrong. all reactions rclcpp/include/rclcpp/experimental/buffers/intra_process_buffer.hpp show resolved hide resolved copy link member christophebedard commented apr 13, 2023 @ymski can you squash the commits into a single commit and rebase on the latest version of the rolling branch? all reactions sorry, something went wrong. applied tracepoints for ring_buffer  4dbeb17 signed-off-by: kodai yamasaki <114902604+ymski@users.noreply.github.com> applied tracepoints for intra_publish signed-off-by: kodai yamasaki <114902604+ymski@users.noreply.github.com> add tracepoints for linking buffer and  signed-off-by: kodai yamasaki <114902604+ymski@users.noreply.github.com> rename buf_to_typedipb signed-off-by: kodai yamasaki <114902604+ymski@users.noreply.github.com> added accumulated data signed-off-by: kodai yamasaki <114902604+ymski@users.noreply.github.com> commit sugesstion signed-off-by: kodai yamasaki <114902604+ymski@users.noreply.github.com> refactor: split long lines signed-off-by: kodai yamasaki <114902604+ymski@users.noreply.github.com> added prefix rclcpp signed-off-by: kodai yamasaki <114902604+ymski@users.noreply.github.com> ymski force-pushed the add-intra-process-tracepoint branch from c94ba1b to 4dbeb17 compare april 13, 2023 02:55 copy link contributor author ymski commented apr 13, 2023 @fujitatomoya  very much for reviewing this pr. i apologize for the delay in responding after submitting the pr. thanks to your feedback, we have been able to make it this far. @christophebedard  for providing additional details  the code. it was very helpful. i have completed the squash and rebase.  1 christophebedard reacted with thumbs up emoji all reactions  1 reaction sorry, something went wrong. copy link member christophebedard commented apr 13, 2023 ci for this pr and ros2/ros2_tracing#30 is over at ros2/ros2_tracing#30 (comment) all reactions sorry, something went wrong. copy link member christophebedard commented apr 13, 2023 ci looks good. i merged the ros2_tracing pr. @fujitatomoya can you merge this one? all reactions sorry, something went wrong. christophebedard mentioned this pull request apr 13, 2023 release 6.1.0 ros2/ros2_tracing#61 merged clalancette merged commit 82a693e into ros2 : rolling apr 13, 2023 barry-xu-2018 pushed a commit to barry-xu-2018/rclcpp that referenced this pull request jan 12, 2024 applied tracepoints for ring_buffer ( ros2#2091 )  43090aa applied tracepoints for intra_publish add tracepoints for linking buffer and  signed-off-by: kodai yamasaki <114902604+ymski@users.noreply.github.com>  for free to join this conversation on github . already have an account?  to comment reviewers christophebedard christophebedard approved these changes fujitatomoya fujitatomoya approved these changes ivanpauno awaiting requested review from ivanpauno ivanpauno is a code owner hidmic awaiting requested review from hidmic hidmic is a code owner wjwwood awaiting requested review from wjwwood wjwwood is a code owner assignees no one assigned labels none yet projects none yet milestone no milestone development successfully merging this pull request may close these issues. 5 participants add this suggestion to a batch that can be applied as a single commit. this suggestion is invalid because no changes were made to the code. suggestions cannot be applied while the pull request is closed. suggestions cannot be applied while viewing a subset of changes. only one suggestion per line can be applied in a batch. add this suggestion to a batch that can be applied as a single commit. applying suggestions on deleted lines is not supported. you must change the existing code in this line in order to create a valid suggestion. outdated suggestions cannot be applied. this suggestion has been applied or marked resolved. suggestions cannot be applied from pending reviews. suggestions cannot be applied on multi-line comments. suggestions cannot be applied while the pull request is queued to merge. suggestion cannot be applied right now.  check back later. you cant perform that action at this time.