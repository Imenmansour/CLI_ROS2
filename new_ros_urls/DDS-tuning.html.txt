how-to guides dds tuning information edit on github dds tuning information ï this page provides some guidance on parameter tunings that were found to address issues faced while using various dds implementations on linux in real-world situations. it is possible that the issues we identified on linux or while using one vendor may occur for other platforms and vendors not documented here. the recommendations below are starting points for tuning; they worked for specific systems and environments, but the tuning may vary depending on a number of factors. you may need to increase or decrease values while debugging relative to factors like message size, network topology, etc. it is important to recognize that tuning parameters can come at a cost to resources, and may affect parts of your system beyond the scope of the desired improvements. the benefits of improving reliability should be weighed against any detriments for each individual case. cross-vendor tuning ï issue: sending data over lossy (usually wifi) connections becomes problematic when some ip fragments are dropped, possibly causing the kernel buffer on the receiving side to become full. when a udp packet is missing at least one ip fragment, the rest of the received fragments fill up the kernel buffer. by default, the linux kernel will time out after 30s of trying to recombine packet fragments. since the kernel buffer is full at this point (default size is 256kb), no new fragments can come in, and so the connection will seemingly âhangâ for long periods of time. this issue is generic across all dds vendors, so the solutions involve adjusting kernel parameters. solution: use best-effort qos settings instead of reliable. best-effort settings reduce the amount of network traffic since the dds implementation does not have to incur the overhead of reliable communications, where publishers require acknowledgements for messages sent to subscribers and must resend samples that have not been properly received. if the kernel buffer for ip fragments gets full, though, the symptom is still the same (blocking for 30s). this solution should improve the issue somewhat without having to adjust parameters. solution: reduce the value of the ipfrag_time parameter. net.ipv4.ipfrag_time / /proc/sys/net/ipv4/ipfrag_time (default 30s) : time in seconds to keep an ip fragment in memory. reduce the value, for example, to 3s, by running: sudo sysctl net.ipv4.ipfrag_time=3 reducing this parameterâs value also reduces the window of time where no fragments are received. the parameter is global for all incoming fragments, so the feasibility of reducing its value needs to be considered for every environment. solution: increase the value of the ipfrag_high_thresh parameter. net.ipv4.ipfrag_high_thresh / /proc/sys/net/ipv4/ipfrag_high_thresh (default: 262144 bytes): maximum memory used to reassemble ip fragments. increase the value, for example, to 128mb, by running: sudo sysctl net.ipv4.ipfrag_high_thresh=134217728 # (128 mb) significantly increasing this parameterâs value is an attempt to ensure that the buffer never becomes completely full. however, the value would likely have to be significantly high to hold all data received during the time window of ipfrag_time , assuming every udp packet lacks one fragment. issue: sending custom messages with large variable-sized arrays of non-primitive types causes high serialization/deserialization overhead and cpu load. this can lead to stalling of the publisher due to excessive time spent in publish() and tools like ros2 topic hz under reporting the actual frequency of messages being received. note that for example builtin_interfaces/time is also considered a non-primitive type and will incur higher serialization overhead. because of the increased serialization overhead, severe performance degradation can be observed when naively transitioning custom message types from ros 1 to ros 2. workaround: use multiple arrays of primitives instead of a single array of custom types, or pack into byte array as done e.g. in pointcloud2 messages. for example, instead of defining a fooarray message as: foo[] my_large_array with foo is defined as: uint64 foo_1 uint32 foo_2 instead, define fooarray as: uint64[] foo_1_array uint32[] foo_2_array fast rtps tuning ï issue: fast rtps floods the network with large pieces of data or fast-published data when operating over wifi. see the solutions under cross-vendor tuning . cyclone dds tuning ï issue: cyclone dds is not delivering large messages reliably, despite using reliable settings and transferring over a wired network. this issue should be addressed soon . until then, weâve come up with the following solution (debugged using this test program ): solution: increase the maximum linux kernel receive buffer size and the minimum socket receive buffer size that cyclone uses. adjustments to solve for a 9mb message: set the maximum receive buffer size, rmem_max , by running: sudo sysctl -w net.core.rmem_max=2147483647 or permanently set it by editing the /etc/sysctl.d/10-cyclone-max.conf file to contain: net.core.rmem_max=2147483647 next, to set the minimum socket receive buffer size that cyclone requests, write out a configuration file for cyclone to use while starting, like so: <?xml version="1.0" encoding="utf-8" ?> <cyclonedds xmlns= "https://cdds.io/config" xmlns:xsi= "http://www.w3.org/2001/xmlschema-instance" xsi:schemalocation= "https://cdds.io/config https://raw.githubusercontent.com/eclipse-cyclonedds/cyclonedds/master/etc/cyclonedds.xsd" > <domain id= "any" > <internal> <socketreceivebuffersize min= "10mb" /> </internal> </domain> </cyclonedds> then, whenever you are going to run a node, set the following environment variable: cyclonedds_uri=file:///absolute/path/to/config_file.xml rti connext tuning ï issue: connext is not delivering large messages reliably, despite using reliable settings and transferring over a wired network. solution: this connext qos profile , along with increasing the rmem_max parameter. set the maximum receive buffer size, rmem_max , by running: sudo sysctl -w net.core.rmem_max=4194304 by tuning net.core.rmem_max to 4mb in the linux kernel, the qos profile can produce truly reliable behavior. this configuration has been proven to reliably deliver messages via shmem|udpv4, and with just udpv4 on a single machine. a multi-machine configuration was also tested with rmem_max at 4mb and at 20mb (two machines connected with 1gbps ethernet), with no dropped messages and average message delivery times of 700ms and 371ms, respectively. without configuring the kernelâs rmem_max , the same connext qos profile took up to 12 seconds for the data to be delivered. however, it always at least managed to complete the delivery. solution: use the connext qos profile without adjusting rmem_max . the ros2test_qos_profiles.xml file was configured using rtiâs documentation on configuring flow controllers . it has slow, medium and fast flow controllers (seen in the connext qos profile link). the medium flow controller produced the best results for our case. however, the controllers will still need to be tuned for the particular machine/network/environment they are operating in. the connext flow controllers can be used to tune bandwidth and its aggressiveness for sending out data, though once the bandwidth of a particular setup is passed, performance will start to drop. other versions v: jazzy releases jazzy (latest) iron humble galactic (eol) foxy (eol) eloquent (eol) dashing (eol) crystal (eol) in development rolling