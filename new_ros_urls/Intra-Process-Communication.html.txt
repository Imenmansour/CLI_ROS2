tutorials demos setting up efficient intra-process communication edit on github setting up efficient intra-process communication ï background ï ros applications typically consist of a composition of individual ânodesâ which perform narrow tasks and are decoupled from other parts of the system. this promotes fault isolation, faster development, modularity, and code reuse, but it often comes at the cost of performance. after ros 1 was initially developed, the need for efficient composition of nodes became obvious and nodelets were developed. in ros 2 we aim to improve on the design of nodelets by addressing some fundamental problems that required restructuring of nodes. in this demo weâll be highlighting how nodes can be composed manually, by defining the nodes separately but combining them in different process layouts without changing the nodeâs code or limiting its abilities. installing the demos ï see the installation instructions for details on installing ros 2. if youâve installed ros 2 from packages, ensure that you have ros-jazzy-intra-process-demo installed. if you downloaded the archive or built ros 2 from source, it will already be part of the installation. running and understanding the demos ï there are a few different demos: some are toy problems designed to highlight features of the intra process communications functionality and some are end to end examples which use opencv and demonstrate the ability to recombine nodes into different configurations. the two node pipeline demo ï this demo is designed to show that the intra process publish/subscribe connection can result in zero-copy transport of messages when publishing and subscribing with std::unique_ptr s. first letâs take a look at the source: https://github.com/ros2/demos/blob/jazzy/intra_process_demo/src/two_node_pipeline/two_node_pipeline.cpp #include <chrono> #include <cinttypes> #include <cstdio> #include <memory> #include <string> #include <utility> #include "rclcpp/rclcpp.hpp" #include "std_msgs/msg/int32.hpp" using namespace std :: chrono_literals ; // node that produces messages. struct producer : public rclcpp :: node { producer ( const std :: string & name , const std :: string & output ) : node ( name , rclcpp :: nodeoptions (). use_intra_process_comms ( true )) { // create a publisher on the output topic. pub_ = this -> create_publisher < std_msgs :: msg :: int32 > ( output , 10 ); std :: weak_ptr < std :: remove_pointer < decltype ( pub_ . get ()) >:: type > captured_pub = pub_ ; // create a timer which publishes on the output topic at ~1hz. auto callback = [ captured_pub ]() -> void { auto pub_ptr = captured_pub . lock (); if ( ! pub_ptr ) { return ; } static int32_t count = 0 ; std_msgs :: msg :: int32 :: uniqueptr msg ( new std_msgs :: msg :: int32 ()); msg -> data = count ++ ; printf ( "published message with value: %d, and address: 0x%" prixptr " \n " , msg -> data , reinterpret_cast < std :: uintptr_t > ( msg . get ())); pub_ptr -> publish ( std :: move ( msg )); }; timer_ = this -> create_wall_timer ( 1 s , callback ); } rclcpp :: publisher < std_msgs :: msg :: int32 >:: sharedptr pub_ ; rclcpp :: timerbase :: sharedptr timer_ ; }; // node that consumes messages. struct consumer : public rclcpp :: node { consumer ( const std :: string & name , const std :: string & input ) : node ( name , rclcpp :: nodeoptions (). use_intra_process_comms ( true )) { // create a  on the input topic which prints on receipt of new messages. sub_ = this -> create_ < std_msgs :: msg :: int32 > ( input , 10 , []( std_msgs :: msg :: int32 :: uniqueptr msg ) { printf ( " received message with value: %d, and address: 0x%" prixptr " \n " , msg -> data , reinterpret_cast < std :: uintptr_t > ( msg . get ())); }); } rclcpp ::  < std_msgs :: msg :: int32 >:: sharedptr sub_ ; }; int main ( int argc , char * argv []) { setvbuf ( stdout , null , _ionbf , bufsiz ); rclcpp :: init ( argc , argv ); rclcpp :: executors :: singlethreadedexecutor executor ; auto producer = std :: make_shared < producer > ( "producer" , "number" ); auto consumer = std :: make_shared < consumer > ( "consumer" , "number" ); executor . add_node ( producer ); executor . add_node ( consumer ); executor . spin (); rclcpp :: shutdown (); return 0 ; } as you can see by looking at the main function, we have a producer and a consumer node, we add them to a single threaded executor, and then call spin. if you look at the âproducerâ nodeâs implementation in the producer struct, you can see that we have created a publisher which publishes on the ânumberâ topic and a timer which periodically creates a new message, prints out its address in memory and its contentâs value and then publishes it. the âconsumerâ node is a bit simpler, you can see its implementation in the consumer struct, as it only subscribes to the ânumberâ topic and prints the address and value of the message it receives. the expectation is that the producer will print out an address and value and the consumer will print out a matching address and value. this demonstrates that intra process communication is indeed working and unnecessary copies are avoided, at least for simple graphs. letâs run the demo by executing ros2 run intra_process_demo two_node_pipeline executable (donât forget to source the setup file first):  ros2 run intra_process_demo two_node_pipeline published message with value: 0 , and address: 0x7fb02303faf0 published message with value: 1 , and address: 0x7fb020cf0520 received message with value: 1 , and address: 0x7fb020cf0520 published message with value: 2 , and address: 0x7fb020e12900 received message with value: 2 , and address: 0x7fb020e12900 published message with value: 3 , and address: 0x7fb020cf0520 received message with value: 3 , and address: 0x7fb020cf0520 published message with value: 4 , and address: 0x7fb020e12900 received message with value: 4 , and address: 0x7fb020e12900 published message with value: 5 , and address: 0x7fb02303cea0 received message with value: 5 , and address: 0x7fb02303cea0 [ ... ] one thing youâll notice is that the messages tick along at  one per second. this is because we told the timer to fire at  once per second. also you may have noticed that the first message (with value 0 ) does not have a corresponding âreceived message ââ line. this is because publish/subscribe is âbest effortâ and we do not have any âlatchingâ like behavior enabled. this means that if the publisher publishes a message before the  has been established, the  will not receive that message. this race condition can result in the first few messages being lost. in this case, since they only come once per second, usually only the first message is lost. finally, you can see that âpublished messageââ and âreceived message ââ lines with the same value also have the same address. this shows that the address of the message being received is the same as the one that was published and that it is not a copy. this is because weâre publishing and subscribing with std::unique_ptr s which allow ownership of a message to be moved around the system safely. you can also publish and subscribe with const & and std::shared_ptr , but zero-copy will not occur in that case. the cyclic pipeline demo ï this demo is similar to the previous one, but instead of the producer creating a new message for each iteration, this demo only ever uses one message instance. this is achieved by creating a cycle in the graph and âkicking offâ communication by externally making one of the nodes publish before spinning the executor: https://github.com/ros2/demos/blob/jazzy/intra_process_demo/src/cyclic_pipeline/cyclic_pipeline.cpp #include <chrono> #include <cinttypes> #include <cstdio> #include <memory> #include <string> #include <utility> #include "rclcpp/rclcpp.hpp" #include "std_msgs/msg/int32.hpp" using namespace std :: chrono_literals ; // this node receives an int32, waits 1 second, then increments and sends it. struct incrementerpipe : public rclcpp :: node { incrementerpipe ( const std :: string & name , const std :: string & in , const std :: string & out ) : node ( name , rclcpp :: nodeoptions (). use_intra_process_comms ( true )) { // create a publisher on the output topic. pub = this -> create_publisher < std_msgs :: msg :: int32 > ( out , 10 ); std :: weak_ptr < std :: remove_pointer < decltype ( pub . get ()) >:: type > captured_pub = pub ; // create a  on the input topic. sub = this -> create_ < std_msgs :: msg :: int32 > ( in , 10 , [ captured_pub ]( std_msgs :: msg :: int32 :: uniqueptr msg ) { auto pub_ptr = captured_pub . lock (); if ( ! pub_ptr ) { return ; } printf ( "received message with value: %d, and address: 0x%" prixptr " \n " , msg -> data , reinterpret_cast < std :: uintptr_t > ( msg . get ())); printf ( " sleeping for 1 second... \n " ); if ( ! rclcpp :: sleep_for ( 1 s )) { return ; // return if the sleep failed (e.g. on :kbd:`ctrl-c`). } printf ( " done. \n " ); msg -> data ++ ; // increment the message's data. printf ( "incrementing and sending with value: %d, and address: 0x%" prixptr " \n " , msg -> data , reinterpret_cast < std :: uintptr_t > ( msg . get ())); pub_ptr -> publish ( std :: move ( msg )); // send the message along to the output topic. }); } rclcpp :: publisher < std_msgs :: msg :: int32 >:: sharedptr pub ; rclcpp ::  < std_msgs :: msg :: int32 >:: sharedptr sub ; }; int main ( int argc , char * argv []) { setvbuf ( stdout , null , _ionbf , bufsiz ); rclcpp :: init ( argc , argv ); rclcpp :: executors :: singlethreadedexecutor executor ; // create a simple loop by connecting the in and out topics of two incrementerpipe's. // the expectation is that the address of the message being passed between them never changes. auto pipe1 = std :: make_shared < incrementerpipe > ( "pipe1" , "topic1" , "topic2" ); auto pipe2 = std :: make_shared < incrementerpipe > ( "pipe2" , "topic2" , "topic1" ); rclcpp :: sleep_for ( 1 s ); // wait for s to be established to avoid race conditions. // publish the first message (kicking off the cycle). std :: unique_ptr < std_msgs :: msg :: int32 > msg ( new std_msgs :: msg :: int32 ()); msg -> data = 42 ; printf ( "published first message with value: %d, and address: 0x%" prixptr " \n " , msg -> data , reinterpret_cast < std :: uintptr_t > ( msg . get ())); pipe1 -> pub -> publish ( std :: move ( msg )); executor . add_node ( pipe1 ); executor . add_node ( pipe2 ); executor . spin (); rclcpp :: shutdown (); return 0 ; } unlike the previous demo, this demo uses only one node, instantiated twice with different names and configurations. the graph ends up being pipe1 -> pipe2 -> pipe1 â in a loop. the line pipe1->pub->publish(msg); kicks the process off, but from then on the messages are passed back and forth between the nodes by each one calling publish within its own  callback. the expectation here is that the nodes pass the message back and forth, once a second, incrementing the value of the message each time. because the message is being published and subscribed to as a unique_ptr the same message created at the beginning is continuously used. to test those expectations, letâs run it:  ros2 run intra_process_demo cyclic_pipeline published first message with value: 42 , and address: 0x7fd2ce0a2bc0 received message with value: 42 , and address: 0x7fd2ce0a2bc0 sleeping for 1 second... done . incrementing and sending with value: 43 , and address: 0x7fd2ce0a2bc0 received message with value: 43 , and address: 0x7fd2ce0a2bc0 sleeping for 1 second... done . incrementing and sending with value: 44 , and address: 0x7fd2ce0a2bc0 received message with value: 44 , and address: 0x7fd2ce0a2bc0 sleeping for 1 second... done . incrementing and sending with value: 45 , and address: 0x7fd2ce0a2bc0 received message with value: 45 , and address: 0x7fd2ce0a2bc0 sleeping for 1 second... done . incrementing and sending with value: 46 , and address: 0x7fd2ce0a2bc0 received message with value: 46 , and address: 0x7fd2ce0a2bc0 sleeping for 1 second... done . incrementing and sending with value: 47 , and address: 0x7fd2ce0a2bc0 received message with value: 47 , and address: 0x7fd2ce0a2bc0 sleeping for 1 second... [ ... ] you should see ever increasing numbers on each iteration, starting with 42â because 42, and the whole time it reuses the same message, as demonstrated by the pointer addresses which do not change, which avoids unnecessary copies. the image pipeline demo ï in this demo weâll use opencv to capture, annotate, and then view images. note if you are on macos and these examples do not work or you receive an error like ddsi_conn_write failed -1 , then youâll need to increase your system wide udp packet size:  sudo sysctl -w net.inet.udp.recvspace = 209715  sudo sysctl -w net.inet.udp.maxdgram = 65500 these changes will not persist after a reboot. simple pipeline ï first weâll have a pipeline of three nodes, arranged as such: camera_node -> watermark_node -> image_view_node the camera_node reads from camera device 0 on your computer, writes some information on the image and publishes it. the watermark_node subscribes to the output of the camera_node and adds more text before publishing it too. finally, the image_view_node subscribes to the output of the watermark_node , writes more text to the image and then visualizes it with cv::imshow . in each node the address of the message which is being sent, or which has been received, or both, is written to the image. the watermark and image view nodes are designed to modify the image without copying it and so the addresses imprinted on the image should all be the same as long as the nodes are in the same process and the graph remains organized in a pipeline as sketched above. note on some systems (weâve seen it happen on linux), the address printed to the screen might not change. this is because the same unique pointer is being reused. in this situation, the pipeline is still running. letâs run the demo by executing the following executable: ros2 run intra_process_demo image_pipeline_all_in_one you should see something like this: you can pause the rendering of the image by pressing the spacebar and you can resume by pressing the spacebar again. you can also press q or esc to exit. if you pause the image viewer, you should be able to compare the addresses written on the image and see that they are the same. pipeline with two image viewers ï now letâs look at an example just like the one above, except it has two image view nodes. all the nodes are still in the same process, but now two image view windows should show up. (note for macos users: your image view windows might be on top of each other). letâs run it with the command: ros2 run intra_process_demo image_pipeline_with_two_image_view just like the last example, you can pause the rendering with the spacebar and continue by pressing the spacebar a second time. you can stop the updating to inspect the pointers written to the screen. as you can see in the example image above, we have one image with all of the pointers the same and then another image with the same pointers as the first image for the first two entries, but the last pointer on the second image is different. to understand why this is happening consider the graphâs topology: camera_node -> watermark_node -> image_view_node -> image_view_node2 the link between the camera_node and the watermark_node can use the same pointer without copying because there is only one intra process  to which the message should be delivered. but for the link between the watermark_node and the two image view nodes the relationship is one to many, so if the image view nodes were using unique_ptr callbacks then it would be impossible to deliver the ownership of the same pointer to both. it can be, however, delivered to one of them. which one would get the original pointer is not defined, but instead is simply the last to be delivered. note that the image view nodes are not subscribed with unique_ptr callbacks. instead they are subscribed with const shared_ptr s. this means the system deliveres the same shared_ptr to both callbacks. when the first intraprocess  is handled, the internally stored unique_ptr is promoted to a shared_ptr . each of the callbacks will receive shared ownership of the same message. pipeline with interprocess viewer ï one other important thing to get right is to avoid interruption of the intra process zero-copy behavior when interprocess s are made. to test this we can run the first image pipeline demo, image_pipeline_all_in_one , and then run an instance of the stand alone image_view_node (donât forget to prefix them with ros2 run intra_process_demo in the terminal). this will look something like this: itâs hard to pause both images at the same time so the images may not line up, but the important thing to notice is that the image_pipeline_all_in_one image view shows the same address for each step. this means that the intra process zero-copy is preserved even when an external view is subscribed as well. you can also see that the interprocess image view has different process ids for the first two lines of text and the process id of the standalone image viewer in the third line of text. other versions v: jazzy releases jazzy (latest) iron humble galactic (eol) foxy (eol) eloquent (eol) dashing (eol) crystal (eol) in development rolling