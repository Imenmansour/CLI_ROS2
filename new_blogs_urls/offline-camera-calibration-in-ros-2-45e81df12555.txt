open in app sign up sign in write sign up sign in offline camera calibration in ros 2 tamas foldi  follow published in hcltech-starschema   4 min read  nov 3, 2023 -- listen share in applications like self-driving cars, robots and augmented reality systems, where precise and reliable data is needed for positioning and overall perception, calibration is crucial. garbage data in, garbage data out, as people say in todays data world  and you can easily imagine the potential consequences of garbage out in the case of an autonomous vehicle or drone. the good old checkboard pattern preparing a computer vision pipeline almost invariably starts with camera calibration. when we calibrate a camera, were collecting information on its own features and how it sees the world. this includes knowing how far it can see and at what angle  for example, we cannot make good use of an image produced by a fisheye-like lens without knowing just how it distorts the image. so, let me show you how i went  calibrating a camera in ros 2 and how you can do the same. online calibration first, we have to collect the ros 2 package for the calibration: # on linux with apt sudo apt install ros-<ros2-distro>-camera-calibration # on mac with robostack mamba install ros-<ros2-distro>-camera-calibration then, we have to ensure we have a camera driver compatible with our camera. i tend to reply in the following packages for these systems: usb_cam on linux using v4l2. it does not work on mac or windows. opencv_cam on mac (or windows) using opencv to capture images from a camera or files. video_source on nvidia jetson devices (supports both cameras and flat files). we also need a pattern to use during the calibration. you can generate one easily here: https://calib.io/pages/camera-calibration-pattern-generator . to fit your pattern on an a4/us letter page, i suggest an 8x10 board with 15mm squares. print it on thick paper to ensure it doesnt bend during the calibration process. an 8x10 checkerboard with 15mm squares is a good option. to start calibration in one console, start the following command: ros2 run camera_calibration cameracalibrator --size 7x9 --square 0.015 \ --ros-args -r image:=/image_raw the size option here denotes interior corners (e.g. a standard chessboard is 7x7), so for an 8x10 checkerboard, we go with 7x9. on the other console, start your camera capture: # in case of opencv_cam driver ros2 run opencv_cam opencv_cam_main you should now see the calibration window and begin the calibration process. next, move the pattern to all screen corners and tilt in every direction. when enough information is gathered, press the calibrate button. try to remember what you do here when switching to offline mode. now you can see your camera calibration data in the console. you can simply save it in a file with ini extension or press the save button in the app to save the same in a tarball with both ini and yaml format. an example of calibration output for my macbook camera. save the contents from image till the end to an ini file. offline calibration the process will bethe same as above, but instead of using your camera image, ingest video files to the /video_raw topic. most camera drivers can do this, but i prefer opencv_cam due to its mac support. first, record a video of you dancing in front of your camera: calibrating my cars cameras is always a fun. then, load the video files using opencv_cam ros2 run opencv_cam opencv_cam_main --ros-args -p file:=true \ -p filename:=/../teslacam/recentclips/2023-11-01_15-07-10-front.mp4 viewing the results you can also use the same opencv_cam command to replay your recordings, but this time, you should also specify the camera_info_path argument for feeding data to the /camera_info topic. this topic uses the camerainfo message to tell upstream systems how to interpret the images from video frames. the easiest way to visualize the difference between the calibrated and non-calibrated images is to open two image windows in foxglove, one with a calibration topic and one without it. the image on the left is the raw image, while the right is the one we can use for further processing. you can easily compare calibrated images and video streams side by side before and after calibration. summary the calibration process i described here is a necessary step in a computer vision pipeline, especially if you will use the images for localization or mapping. ros 2 supports this project out of the box with a calibrator, camera info messages, services and visualization tools. reach out to us here to learn more:  let's talk starschema.com read more stories from starschema: collecting can bus data with ros2 and qt get started with vehicle telemetry with this practical deep dive into sensor fusion and data collection at the edge. medium.com mqtt on steroids: running emqx enterprise on kubernetes learn how to deploy emqx, a robust mqtt message broker in your cloud or an on-prem kubernetes cluster with a single medium.com avoid these 3 mistakes when working with a synapse dedicated sql pool learn three techniques for optimizing resource utilization and ensuring cost-effectiveness with a synapse dedicated sql medium.com data science ros2 computer vision robotics follow written by tamas foldi 304 followers  editor for hcltech-starschema  helping enterprises to become more data driven @ hcltech, co-founder & former ceo @ starschema follow help status  careers press    text to speech teams