skip to content this  post presents some issues which came up when we migrated from ros 1 and started using ros 2. we assume that other people might run into them too. therefore, we will describe the problems and possible solutions below. our use case of ros 2 is a bit different than most systems that currently use ros 2. we use a team of humanoid robots in robocup soccer. therefore we have on the one hand differences from using a humanoid robot (500+ hz control loop cycle) and on the other hand from the league (no network connection to the robot after starting the code). scheduling we have many nodes (~45) running concurrently when we start our full software stack. most of them have additionally multiple threads. at the same time, some of them, e.g. the walking, need to be executed with a constant and high (500+ hz) rate. this leads to some issues with the default linux scheduler. setting the nice value of the processes was not enough to solve this because it does not necessarily reduce latencies. in ros 1 it was enough for us to assign the processes to specific cpus using the taskset command, but with ros 2 we still had issues with this. we finally resolved it by also using the isolcpus kernel parameter which forbids the scheduler of using the specified cpu core. therefore, the process can run freely on its own cpu without any interruptions. using a real time kernel would probably be a better, but also more complicated solution that we will investigate in the future. more information can be found in the following roscon talk: https://roscon.ros.org/2019/talks/roscon2019_concurrency.pdf executor performance one of our largest issues was the extreme performance drop between ros 1 and ros 2. simple nodes that only took a few percent of a core before, now needed a complete core for themselves. basically, almost every node was running at 100% cpu usage. it took us some time to realize that the issue comes from the fact that we have a lot of messages per second (e.g. /joint_states is sent with 500 hz) and thus the badly implemented standard executor was totally overloaded. luckily, irobot already did a lot of work on this and created an events executor for rclcpp ( https://github.com/ros2/design/pull/305 ). unfortunately, this executor is not yet in the master. furthermore, there is no such thing for rclpy yet. by using it, we reached node performance values similar to ros 1 with our c++ nodes. but we also needed to rewrite some nodes from python to c++ as it was otherwise not possible to run our complete software stack on our 8 core cpu. unfortunately, all standard ros 2 nodes, e.g. robot state publisher, use the default implementation and therefore needed to be manually patched by us. this also includes the tf listener. the repository containing the stand-alone events executor can be found here: https://github.com/irobot-ros/events-executor/ our patched versions of rclcpp and other packages are linked here: https://github.com/ros2/design/pull/305#issuecomment-1133757777 callbacks and timers when you are in a callback or timer thread (so anything that is handled by the spinning), it is not possible to receive other callbacks per default. this means for example that you can not wait to get a tf transformation, as you will never receive anything while waiting for it. the same is the case for simulation time callbacks in rclpy. this leads to the time not progressing inside callbacks which leads to other issues. the issue is solved in rclcpp by spinning a callback group containing the time callback in a separate thread, but the isolated execution of specific callback groups is not supported in the current implementation of the python executor (see https://github.com/ros2/rclpy/issues/850 ). although there are multi-threaded executors available, they seem to not solve the issue completely. if there are many callbacks to handle, they might not manage to handle the correct one in time before you run into a timeout. interestingly, the order in which the subscriptions are created influences this behavior and sometimes issues can be resolved by ordering them differently. fastdds sometimes fastdds fails to list nodes / topics after restarting a node while other nodes are running. the fastdds discovery server (see https://fast-dds.docs.eprosima.com/en/latest/fastdds/ros2/discovery_server/ros2_discovery_server.html ) is similar to the concept of a rosmaster in ros 1 and should fix this issue. there are issues with callbacks not arriving in c++ on ros 2 rolling under ubuntu 22.04 and fastdds, which was reintroduced as the default dds for rolling. we observed these issues our self in our own code base, but they are also the reason why nav2 is not released for rolling. see https://github.com/ros-planning/navigation2/issues/2648 for information regarding the release of nav2 for rolling and humble as well as https://discourse.ros.org/t/nav2-issues-with-humble-binaries-due-to-fast-dds-rmw-regression/26128 and https://discourse.ros.org/t/fastdds-without-discovery-server/26117/14 for a more general discussion. switching to cyclonedds solved these issues for us, but we still need to build nav2 ourselves. cyclonedds configuration cyclone allows a lot of further settings. unfortunately, this is just documented in the cyclone documentation. one important setting when working with large messages, e.g. images, is the kernel queue size (see https://github.com/ros2/rmw_cyclonedds ). for cyclone configuration related things one needs to go to https://cyclonedds.io/docs/cyclonedds/latest . to activate cyclonedds follow the steps on https://github.com/ros2/rmw_cyclonedds . the biggest problem that we were facing with cyclone is that it can only operate on a fixed network configuration which cannot be changed at runtime. the default network adapter that is used it determined by a ranking. wired adapters have the highest priority, loopbacks the lowest. adapters can also be specified manually in the cyclone config file. issues arise if the selected adapter is not available, e.g. because the ethernet cable is unplugged from the robot after starting the software. in our case, that is quite often the case because an ethernet cable for debugging is removed or the wifi disconnects. we solved this issue by creating a bridge network containing the wired adapter. this results in the adapter being available even when the cable is unplugged. cyclone is then configured to use this bridge network adapter. there is a fix already proposed for cyclonedds which enables optional adapters, but it is not merged at the time of writing (see https://github.com/eclipse-cyclonedds/cyclonedds/pull/1336 ). it will also only partially fix the problem, as cyclone will continue to try to use the adapter that was connected when it was first started, and not dynamically switch to another one. as our robot uses the builtin ethernet adapter internally for the camera, external devices such as laptops are connected via a usb2lan adapter. this introduced the problem that its network interface name might change due to different brands etc. we mitigate this by only using adapters from a single brand and adding a udev rule that always assigns the same name to it. it is then included by this name in the bridge interface described above. ros domain id while the default in ros 1 was that you use your local roscore, in ros 2 the default is to communicate with anybody in the network. this can quite quickly lead to issues if you have multiple robots or workstations running in the same network. therefore it is crucial to set the ros_domain_id environment variable differently for every machine, as isolation is done as opt-in instead of opt-out like in ros 1. the number of domain ids is also quite limited for large setups and a ros domain with many nodes might leak into the namespace of another domain id. colcon one of the most unnecessary issues which could have been all avoided by keeping catkin as a build tool are the issues with colcon. just to get the same quality of usage that you had with catkin tools by default you need to invest a lot of effort. first, there are many additional packages that you need to install (colcon-common-extensions, colcon-edit, colcon-clean). then you need to set the verbosity level of colcon, as you will otherwise get completely spammed with unnecessary printouts ( export colcon_log_level=30 ). colors are still not supported although there are prs for it ( https://github.com/colcon/colcon-core/pull/487 ). you might want to create a lot of aliases because the colcon commands are very verbose compared to catkin (e.g. colcon build --packages-up-to package --symlink-install to build a specific package including its present dependencies). rebuilds due to changes in the build process, notably the removal of catkins devel folder, a default build now installs to the install folder, and the install folder has to be sourced. this makes it necessary to rebuild your package for every change you do, even in configuration files, launch files, scripts, or python code. colcon has an option --symlink-install , but it often cannot be relied on and it does not include things like launch files or config files. so get used to rebuilding for every parameter you change in a config file. rate in simulation when using rate.sleep() in simulation, the node does not always sleep correctly. sometimes it does not sleep at all, which leads to executing the code again in the same time step. https://github.com/ros2/rclcpp/issues/465 sim time the usage of lower than real-time simulations (such as https://humanoid.robocup.org/hl-vs2022/ ) can be tricky due to packages using the walltime instead of the ros time for some timeouts etc.. the use_sim_time parameter which tells a node to use the time from the /clock topic instead of the wall time is now present at each node separately. this is the case because the concept of global parameters does not exist in ros 2. it is therefore necessary to pass a launch file argument, e.g. sim:=true , through the full launch file hierarchy to set the use_sim_time parameter at the launch of each node individually. parameters ros 2 handles parameters quite differently compared to ros 1. parameters exist only in the scope of a single node. they need to be declared in the code to be available. during this declaration it is also possible to define the type, value ranges etc. the declaration process is quite verbose if you have a large number of parameters. you can use the following trick to skip the declaration if you dont care  a neatly generated rqt reconfigure gui: for python: node = node("example_node", automatically_declare_parameters_from_overrides=true) # or if you inhert from the node class class mynode(node): def __init__(self): super().__init__('example_node', automatically_declare_parameters_from_overrides=true) for c++: mynode::mynode(const std::string &ns, std::vector<rclcpp::parameter> parameters) : node(ns + "my_node", rclcpp::nodeoptions().allow_undeclared_parameters(true).parameter_overrides(parameters).automatically_declare_parameters_from_overrides(true)) { if you have any global parameter values, a blackboard node which holds these parameters is needed. it could look like this: <node name="parameter_blackboard" pkg="demo_nodes_cpp" exec="parameter_blackboard" args="--ros-args --log-level warn"> <param name="use_sim_time" value="(var sim)"/> <param from="(find-pkg-share my_config_package)/config/global_parameters.yaml" /> </node> retrieving the global parameters from that blackboard is not as straightforward as querying the parameters of your local node. you need to perform a manual service call. note that depending on the executor setup service calls might timeout or block indefinitely when done inside of callbacks. here is some demo code to retrieve the params of another node like the mentioned parameters blackboard. def get_parameters_from_other_node(own_node: node, other_node_name: str, parameter_names: list[str], service_timeout_sec: float = 20.0) -> dict: """ used to receive parameters from other running nodes. returns a dict with requested parameter name as dict key and parameter value as dict value. """ client = own_node.create_client(getparameters, f'{other_node_name}/get_parameters') ready = client.wait_for_service(timeout_sec=service_timeout_sec) if not ready: raise runtimeerror(f'wait for {other_node_name} parameter service timed out') request = getparameters.request() request.names = parameter_names future = client.call_async(request) rclpy.spin_until_future_complete(own_node, future) response = future.result() results = {} # received parameter for i, param in enumerate(parameter_names): results[param] = parameter_value_to_python(response.values[i]) return results tf node spam by default in c++ a tf listener creates its own node just to listen to tf updates in parallel. this is not needed anymore, as rclcpp supports spinning of specific callback groups using their own executor. to lower the overhead and reduce spam in e.g. the rqt node view we therefore suggest that you should replace the following instantiation of the tf listener tf2_ros::buffer tfbuffer{node->get_clock()}; tf2_ros::transformlistener tflistener{tfbuffer}; with this one tf2_ros::buffer tfbuffer{node->get_clock()}; tf2_ros::transformlistener tflistener{tfbuffer, node}; the spinning of the callback group is done with the default executor. you would need to patch tf2_ros your self to use the more performant eventsexecutor. this is especially relevant as /tf callbacks might arrive at high frequency. all of this does not apply to python, as neither the separate callback groups nor the eventsexecutor are implemented for it unreleased packages one thing that is not a major deal breaker but still annoying are missing releases for some packages. there are always some packages, e.g. rqt_tf_tree , that are not released as apt packages (for rolling) although their code works fine. it just seems like there is a lack of maintainers for ros 2. we hope that this changes when support for ros 1 runs out and people can concentrate on ros 2. environment setup there are some things that you can set in your ~/.bashrc or ~/.zshrc : # reduce colcon spam export colcon_log_level=30 # make logs colorful export rcutils_colorized_output=1 # format logs in terminal export rcutils_console_output_format="[{severity}] [{name}]: {message} ({function_name}() at {file_name}:{line_number})" # force using cyclonedds as fastdds is currently broken export rmw_implementation=rmw_cyclonedds_cpp # source ros source /opt/ros/rolling/setup.zsh # source workspace source home/colcon_ws/install/setup.zsh # enable autocompletion eval "(register-python-argcomplete3 ros2)" eval "(register-python-argcomplete3 colcon)" # reduce depraction warning spam in colcon export pythonwarnings='ignore:::setuptools.command.install,ignore:::setuptools.command.easy_install,ignore:::pkg_resources' # always set a domain id (should be unique in your network) export ros_domain_id=42 6 comments hey, thats a great article, we faced almost all of the same issues. i think it would be beneficial to post that on ros discourse, it might get some traction reply thanks for the suggestion! we created a post: https://discourse.ros.org/t/experiences-with-ros-2-on-our-robots-and-what-we-learned-on-the-way/26637 reply thanks for the great article! i just want a clarification  colcon spamming unnecessary printouts, what exactly does it spam out? if there are no warnings or errors during colcon build, it should print out very minimal information. are youre referring to the warning  overriding packages when you have overlayed packages in workspaces? reply we observed some deprecation warnings as well as printout related to different ament env variables. some of that could also be related to our configuration. the default cli output is indeed pretty okay. in addition to that, it is discouraged to build in sourced terminals iirc. which also produces a bunch of printout and may introduce some weird behavior. but i am by no way an expert regarding colcon and ament. reply thanks for sharing! as someone that hasnt made the move to ros2 yet, and its concerned  the little things that make you need to benchmark and triple check everything before you realize its not your code but the underlying framework i really thank you! specially because i really enjoy using python with ros and it sounds half baked in ros2. reply pingback: ros 2 latencies for high frequencies using the eventexecutor  hamburg bit-bots leave a reply cancel reply your email address will not be published. required fields are marked * comment * name * email * website