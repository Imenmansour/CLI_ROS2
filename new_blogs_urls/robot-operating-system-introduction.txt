home getting started subscribe installation getting started with opencv pytorch tensorflow & keras opencv university cvdl master program mastering opencv with python fundamentals of cv & ip deep learning with pytorch deep learning with tensorflow & keras computer vision & deep learning applications mastering generative ai for art cv4faces [enrolled users] (old) free courses tensorflow keras course free opencv course free python for beginners free resources ai consulting  youtube x skip to primary navigation skip to main content skip to primary sidebar skip to footer home > robotics > introduction to ros 2 (robot operating system 2): tutorial on ros 2 working, dds, ros 1 rmw, topics, nodes, publisher, subscriber in python ros (robot operating system) is more than a decade old open-source robotics middleware software, initially developed by two phd students from stanford university . fast-forward to 2024, ros has evolved into a rich ecosystem of utilities, algorithms, and sample applications, transcending its origins as middleware software, and is now used by millions of people and thousands of companies worldwide. ros 2 was announced at roscon 2014 to address challenges like security, reliability in diverse environments, and support for large-scale embedded systems, prompting a complete rebuild of ros1 from the ground up. currently, one of the stable releases of ros 2 is humble hawksbill released in may 2022, however, the latest ros 2 version is jazzy jalisco released in may 2024. ros 2 has support for robotics perception, planning, control as well as simulations. thus, apart from hardware design and setup, everything from hardware integration to building a complete autonomous stack can now be accomplished single-handedly using ros. so, as a robotics developer,its very important to understand how ros works and how to use it. this is the 4th article of the robotics article series . in the previous articles we discussed monocular slam using opencv python and ros 2 carla setup guide . in this  post, we aim to provide an in-depth understanding of ros 2, including its internal workings and key components such as nodes, topic publishers, subscribers, services, and clients, culminating in a small ros 2 project where we integrate monocular slam code from the previous article with ros 2. a thorough understanding of ros 2 is crucial, as we will utilize it in our final project of this  post series: building a 6dof autonomous vehicle for static obstacle avoidance using ros 2 and carla . nb: ros 2 installation assumption: we assume that you have already installed ros 2 in your operating system. if not, please follow the ros 2 installation guide for ubuntu 22.04. python for ros 2: in this ros 2 tutorial, we will exclusively use python to demonstrate all the ros 2 functionalities. we look forward to covering c++ in the upcoming articles, so be sure to follow the series . role of dds in robot operating system 2 what is dds? development in autonomous vehicles industry evolution in networking how dds is being used in ros 2? ros1 rmw architecture working of ros 2 dataflow from os to dds to ros 2 how to use ros 2 in python workspace packages nodes topic subscriber publisher service and client integrate slam with ros 2 in python video frame publisher ros 2 monocular slam code ros 2 important commands key takeaways conclusion references role of dds in robot operating system 2 what is dds? development in autonomous vehicles industry to understand what dds (data distribution service) is and why it is needed, we should start by looking at the evolution of autonomous vehicles. there was a time when vehicles were driven manually. as time progressed, more electronic components were added, bringing automation to everyday vehicles. as the level of autonomy increased, more sensors were added to the vehicles, and the amount of data generated also increased, creating a need for better communication mediums. figure 1: autonomous vehicle sensor setup ( source ) autonomous vehicles generate a massive data flow due to the increasing number of sensors added for improved decision-making and to avoid unexpected scenarios. according to a stanford study, a single autonomous vehicle generates 5tb of data per hour, amounting to 100tb per day. contrary to popular belief, this amount of data cannot be processed in the cloud through 5g, as 5g supports a range of 100 mbps (at the cell edge) to 10 gbps (theoretical). the data varies in size and frequency; some is high volume but low frequency, while others are low volume but high frequency, or a mix of both. for many messages, it is crucial to preserve all data points, but for some, packets can be dropped. this necessitates a wide range of quality of service for autonomous vehicles. to handle this we need a single piece of technology that can handle this entire range of dataflow and qos. evolution in networking we have understood how automation vehicle and robotics industry has progressed over the years now lets understand how the field of networking has progressed. figure 2: networking protocols point to point: ensures the reliable delivery of messages between two endpoints. eg: tcp sockets. client and server focuses on lightweight, efficient message exchange between clients and a central broker. eg: mqtt etc. publisher and subscriber broadcasts messages across a network where devices selectively listen for relevant messages. eg: can bus etc. queuing provides flexible messaging patterns, including queuing, where the focus is on the efficient handling of messages within a distributed system. eg: nats.io etc. while traditional messaging systems like tcp, mqtt, canbus, and nats.io focus primarily on the reliable delivery of messages with the payload being a secondary concern, dds places data at the center of its architecture. by managing dataflow and providing robust qos policies, dds ensures that the right data is always available to the right consumers at the right time, making it a powerful solution for complex, real-time, and data-intensive applications. figure 3: dds databus in autonomous vehicle above is how the dds connectivity framework looks like in an autonomous vehicle. on the left we have planning and control modules and the cloud we have the hdmap information connected by a dds databus. a dds (data distribution service) databus (coloured in orange) is a communication model that facilitates the seamless and real-time exchange of data between distributed systems and applications. in the bottom left we have the sensory input and sensor fusion and localization connected to the databus. to communicate with the vehicle, vehicle platform such as can bus etc. as well as the visualization and navigation components is connected with the databus. dds creates the databus that all the services need to be attached with it for either sharing the captured/estimated information or to receive the shared information. how dds is being used in ros 2? ros (robot operating system) is not an operating system, its a middleware framework having a rich ecosystem. the main value of ros lies in its, tools, capabilities and ecosystem. but the component that was holding the original ros back, was its middleware architecture and in the backend how node management, publisher/subscriber, rpcs were designed and its scalability. the same has been said by the folks from open robotics, ros 1 rmw architecture as commercial opportunities transitioned into products, ross foundation as a research platform began to show its limitations. security, reliability in non-traditional environments, and support for large scale embedded systems became essential to push the industry forward.  robot operating system 2: design, architecture, and uses in the wild this diagram represents the architecture of the robot operating system 1 (ros 1) communication infrastructure, specifically focusing on how nodes (the basic building blocks of ros-based robot control systems) communicate with each other. figure 4: ros1 rmw architecture master node & parameter server: the master node is a central node that is responsible for other nodes to find each other and establish communication. it maintains a registry of all active nodes, topics, and services. the parameter server is used to store and retrieve parameters at runtime. xml-rpc (http): xml-rpc is a remote procedure call (rpc) protocol that uses xml to encode its calls and http as a transport mechanism. this protocol is used for the discovery data phase where nodes register themselves with the master node, and it allows nodes to query the master for information  other nodes, topics, and services. ros tools and user processes: these are tools provided by ros to facilitate various operations like visualization, logging, debugging, etc. these are the actual ros nodes created by users, which perform specific tasks such as controlling sensors, actuators, or processing data. tcp (transport layer protocol) tcp (transmission control protocol) is used for the message data phase, where the actual contents of the messages are transmitted between nodes. this protocol ensures reliable, ordered, and error-checked delivery of data between nodes. working of ros 2: at its core, ros 2 is a middleware framework, and its functionality is fundamentally based on this concept. lets take a simple example, say you have a webcam attached via usb port to your machine, and now you want to access the camera feed. when you connect the webcam to your machine, the operating system recognizes the device and assigns a device file (e.g., /dev/video0 ). to interface with the hardware, youll need a camera driver node, such as v4l2 in linux . this is where a specific ros 2 package, like usb_cam or ros2_usb_camera , comes into play. heres how they work, initializing the camera using the appropriate driver (e.g., v4l2 for linux). opening the device file and starting capturing frames from the webcam. converting the raw frame format to rgb image format and creating a ros 2 message using the image. publishing the ros 2 message to a specific topic (eg: /camera/image_raw ) dataflow from os to dds to ros 2 but, you might wonder what the role of dds is, how dds is connected with the os, and how dds communicates with ros 2? for understanding this we need to trace the data flow through each layer of the architecture, hardware layer: this layer provides an interface to the camera via a usb port. operating system layer: the operating system (linux, windows, or macos) recognizes the webcam and assigns it a device file (e.g., /dev/video0 on linux). the os provides low-level drivers to interface with the webcam hardware. transport layer: the transport layer manages the data transmission between the hardware and higher layers. in this case, it involves usb communication managed by the os. dds implementation layer: dds handles the reliable transmission of messages (camera feed) between ros 2 nodes. one of the dds implementations (fast dds, cyclone dds, or connext dds) is chosen based on the system configuration. figure 5: ros 2 dataflow ros abstract middleware (rmw) layer: this layer ensures that the ros 2 application code can interact with the underlying dds without depending on its specific implementation details. ros client library (rcl) layer: the rcl (ros client library) layer provides core ros 2 functionalities such as node management, topic publishing/subscribing, and service calls. the application interacts with the camera through this library. language-specific client libraries: developers use different languages to program a robot, such as c++, python, or java. based on the language, ros 2 has provided libraries to connect with the ros client library. application code: in this stage we create our own packages and nodes to communicate with the attached sensor. how to use ros 2 in python now, you might wonder what is a package, node, subscriber, publisher, service, client etc? lets understand these ros 2 internals one by one. ros 2 has four ways to communicate: messages: publisher/subscribers are used to share messages via specified topic. services: request/reply model for remote procedure calls. actions: goal/feedback/result model for long-running tasks. feedback is the feedback of the request, and its progress. parameters: modify global data using a request/reply pattern. we will be explaining the first two in the article to follow. workspace robot operating system 2 workspace is the environment where you create your ros 2 packages. these packages have nodes for different functions. this is the same as a python environment . below is how a ros 2 workspace is created,  mkdir -p ros2_ws/src ros 2 workspace is nothing but a folder which contains a sub folder src . generally, src folder contains the source code for ros 2 packages. after you compile the packages using colcon build you will see, there are three more folders, build , log and install . build folder contains compiled files, log folder stores log files and debugging information install folder holds the final installed files and executables ready for deployment. packages in ros 2, you can create a package that contains python nodes, c++ nodes, or a combination of both. say, you have an autonomous vehicle designed to navigate from one place to another avoiding any static obstacles. to do that you can have a separate package for integrating hardware sensors, perception, planning and control. so, based on the use cases you decide how many packages to create and how to design the nodes. treat ros 2 packages like python libraries with different functionalities. if you want to create a package in python use the below commands,  cd ros2_ws/src  ros2 pkg create <package_name> --build-type ament_python the only change in both the commands is the build type for c++ package we use ament_cmake and for python it is ament_python . nodes nodes in ros 2 are responsible to connect with the ros client library and communicate with the hardware and software. nodes in ros 2 are redesigned from ros1. node is a part of the rcl library, offering enhanced modularity, flexibility and performance. by leveraging advanced features like lifecycle management, dds integration, qos policies , and built-in security, ros 2 nodes enable developers to build sophisticated and reliable robotic systems. whether you are a seasoned robotics engineer or a newcomer to the field, understanding and utilizing ros 2 nodes is essential for creating the next generation of robotic applications. generally there are two types of ros 2 nodes, standard node lifecycle management node lifecycle management node ros 2: we will see how to implement a ros 2 standard node in the ros 2 project section. right now, lets check out how to implement a lifecycle node . an example of a library where a lifecycle node is being implemented is the nav2 navigation library . in nav2, lifecycle nodes are used to ensure that the stack can successfully execute without missing any node. generally, the standard node has only two states inactive and active . but, lifecycle node has 4 states, unconfigured , inactive , active , and finalized . figure 6: lifecycle management node ros 2 when a node is created, it exists but is not ready to perform any tasks; this is the unconfigured state. from this state the node can only transition to an inactive state via configured state or finalize via shutdown. in an inactive state the node is configured and can be introspective but does not perform any process. from here the node can transition to activated or unconfigured via activate() or cleanup() transitions respectively. when in the active state the node is fully functional and able to publish or subscribe to topics, provide services. from here the node can transition to inactive or finalized state if deactivate or shutdown transition is invoked. this is the terminal state. the node is no longer operational and has released any resources it used. from this state node doesnt transition to another state. now that we understand the underlying concepts of a lifecycle node, lets implement it. create a python file named lifecycle_node.py under the src/<package_name>/<package_name>/ folder. import rclpy from rclpy.lifecycle import state, transitioncallbackreturn, lifecyclenode from std_msgs.msg import string import sys class lifecyclenodeexample(lifecyclenode): def __init__(self) -> none: super().__init__("lifecycle_publisher") self.publisher_ = self.create_lifecycle_publisher(string, 'topic', 10) self.timer = none self.i = 0 self.mock_file = none # placeholder for a mock resource def on_configure(self, state: state) -> transitioncallbackreturn: self.get_logger().info(f"node '{self.get_name()}' is in state '{state.label}'. transitioning to 'configure'") return transitioncallbackreturn.success def on_activate(self, state: state) -> transitioncallbackreturn: self.get_logger().info(f"node '{self.get_name()}' is in state '{state.label}'. transitioning to 'activate'") timer_period = 0.5 # seconds self.timer = self.create_timer(timer_period, self.timer_callback) # creating a mock resource (e.g., opening a file) self.mock_file = open('/tmp/mock_file.txt', 'w') self.mock_file.write("this is a mock file resource.\n") self.get_logger().info("mock file resource created.") return transitioncallbackreturn.success def on_deactivate(self, state: state) -> transitioncallbackreturn: self.get_logger().info(f"node '{self.get_name()}' is in state '{state.label}'. transitioning to 'deactivate'") if self.timer: self.timer.cancel() return transitioncallbackreturn.success def on_shutdown(self, state: state) -> transitioncallbackreturn: self.get_logger().info(f"node '{self.get_name()}' is in state '{state.label}'. transitioning to 'shutdown'") if self.timer: self.timer.cancel() # cancel the timer to stop publishing messages # additional cleanup for mock resource if self.mock_file: self.mock_file.write("shutting down. cleaning up resources.\n") self.mock_file.close() self.get_logger().info("mock file resource cleaned up.") return transitioncallbackreturn.success def timer_callback(self): msg = string() msg.data = f'hello, world: {self.i}' self.publisher_.publish(msg) self.get_logger().info(f'publishing: {msg.data}') self.i += 1 def main(args=none) -> none: rclpy.init(args=args) lifecycle_node = lifecyclenodeexample() rclpy.spin(lifecycle_node) rclpy.shutdown() if __name__ == "__main__": main() in this node file, we declared a lifecyclepublishernode class inherited from lifecyclenode , and defined a method for each of the nodes transitions between its 4 states. each method must return either failure or success . to run the above code, follow the below instructions, include this node in the entry_points of setup.py file: entry_points={ 'console_scripts': [ "node_name = <package_name>.lifecycle_node:main", ], } run the above script using the executable, and check the current state of the lifecycle node. # in new terminal  ros2 run <package_name> lifecycle_publisher # in new terminal run below commands  ros2 lifecycle list lifecycle_publisher # transition to configure state  ros2 lifecycle set /lifecycle_publisher configure # transition to activate state  ros2 lifecycle set /lifecycle_publisher activate # transition to deactivate state  ros2 lifecycle set /lifecycle_publisher deactivate # transition to shutdown state  ros2 lifecycle set /lifecycle_publisher shutdown if you are unable to configure please dont worry, because in the project part of this ros 2 tutorial we will see how to create a python ros 2 package and configure it. topic topics are a primary method for transferring data between nodes, enabling communication across different parts of the system. topics are represented as /topic_name , and have a message type. say you are sending an image to this topic, then the topic type will be image . similarly, you can create custom message types and have the topic receive that message. figure 7: ros 2 topic animation subscriber subscriber is a node component that receives messages from a specified topic. lets go back to the previous usb camera example. say you have installed usb_cam and after running that package, you have a topic /camera/image_raw where the camera frames will be sent. if you want to use those frames for performing certain tasks, how would you access these topics? well, this can be done using a subscriber. below is the code details, self.subscription = self.create_subscription(string,'/camera/image_raw', self.listener_callback, 10) self.create_subscription is an inherited function from node class . the arguments are message type (here, string), topic name (here, /camera/image_raw ), callback function ( self.listener_callback ) and queue size (here, 10). use of the callback function is to interpret the message and take the useful information from the message. publisher a publisher is a node component that sends messages to a specific topic. we have understood how to get data from a topic, now lets understand how to send our own data to a topic. in ros 2, publishing data to a topic involves several steps under the hood. when a node creates a publisher for a specific topic, the ros 2 middleware (typically based on dds  data distribution service) handles the setup of communication channels. when the publish() method is called on the publisher, the message is serialized into a binary format suitable for transmission. this serialized data is then handed off to the dds layer, which delivers the message to all subscribers of the topic. dds handles the discovery of subscribers , ensuring that messages are routed efficiently. it also applies any configured quality of service (qos) policies, such as reliability and durability, to manage how messages are delivered and stored. the entire process is designed to be transparent to the user, providing a robust and flexible communication mechanism for ros 2 applications. self.publisher_ = self.create_publisher(string, '/topic', 10) here, self.create_publisher is the inherited topic from the node class, string is the message type of the topic, and /topic is the topic name and 10 is the queue size. service in ros 2, a service is a communication mechanism that enables synchronous, request-response interactions between nodes. unlike topics, which are used for continuous, asynchronous data streams, services are designed for tasks that require a request followed by a response. a service consists of a server and a client: the server provides the service, defining the logic to process requests, while the client sends requests to the server and waits for responses. each service has a specific type, defined by a pair of message types (one for the request and one for the response). when a client makes a request, the message is sent to the server, which processes it and sends back a response. this pattern is useful for operations that need confirmation or specific results, such as querying the state of a robot or commanding it to perform a specific action. the underlying dds middleware ensures reliable communication and applies qos policies to manage the interaction between clients and servers efficiently. self.srv = self.create_service(addtwoints, 'add_two_ints', self.add_two_ints_callback) the above line creates a ros 2 service named add_two_ints with the type addtwoints and sets the callback function self.add_two_ints_callback to handle incoming service requests. integrate slam with ros 2 in python finally we arrived at the capstone ros 2 project, where we integrate monocular slam with ros 2. as mentioned, for any ros 2 project we first need to create a workspace. that workspace generally has multiple packages, each package having multiple nodes and each node having multiple functionality. so, lets create a workspace, package and nodes. in this ros 2 tutorial we will see how to wrap the slam code as a ros 2 python package. workspace creation:  mkdir -p /py_ws/src py_ws is the workspace folder and src is the place where the packages will reside. now that we know how to create a workspace, lets create a package. package creation:  cd /py_ws/src  ros2 pkg create slam --build-type ament_python first you get inside the src folder and then create the package slam . ros2 pkg create is used to create the package, --build-type is used to mention the build system, we will pass ament_python because this will be a python package. if it was c++ package or a package containing both c++ and python nodes then we would have used ament_cmake . package file structure: after you create the package, the file structure inside the package slam will look like below. slam  package.xml  resource   slam  setup.cfg  setup.py  slam   __init__.py  test  test_.py  test_flake8.py  test_pep257.py lets understand the file structure under the slam folder. package.xml : this file is used to define the package metadata and dependencies. resource : in a ros 2 package, the resource folder is used to store non-source code files that are essential for the packages functionality. these files can include configuration files, launch files, urdf models, maps, datasets, and other resources that the package needs to operate correctly. setup.cfg : setup.cfg file in the ros 2 package is used to help manage in organizing package configuration, and also to simplify the setup.py . setup.py : it is a standard file in python packages that uses the setuptools library to provide instructions for tasks such as, specifying entry points, dependency management, specifying build configuration etc. slam : this folder contains the source code for the ros2 nodes. __init__.py is generally empty. test : these scripts inside the test folder, help maintain high code quality, consistency, and clear documentation. now, that we understand the utility of each of these files and folders, lets use them to write nodes for following tasks, vod_img_pub_node : publishes the video frames to /cam/image_raw topic. slam_pub_node : publishes the 3d map points and the camera trajectory to /cam/map_points and /cam/camera_pose topic. the feature extraction image is published to /cam/feature_ext_viz topic. figure 8: monocular slam with ros 2 python workflow we will create two files, vod_pub.py and slam_pub.py . vod_pub.py will contain the code for video frame publishing, and slam_pub.py will contain the code for publishing the slam code output. you dont need to worry, we will provide all of this in a structured manner. download the code using the below button, download code to easily follow along this tutorial, please download code by clicking on the button below. it's free! download code click here to download the source code to this post video frame publisher lets start with, vod_pub.py . below is the code for video frame publishing, # import the necessary libraries import cv2 import rclpy from rclpy.node import node import numpy as np from sensor_msgs.msg import image from cv_bridge import cvbridge, cvbridgeerror import sys # define a class for publishing camera images as ros 2 messages class vodimagepublisher(node): def __init__(self, capture): # node name is mentioned here, 'vod_img_pub' super().__init__('vod_img_pub') # create a publisher for the '/cam/image_raw' topic with a queue size of 1 self.publisher_ = self.create_publisher(image, '/cam/image_raw', 1) # a timer is created with a callback to execute every 0.5 seconds timer_period = 0.1 self.timer = self.create_timer(timer_period, self.image_callback) # initialize the image counter self.i = 0 # store the video capture object self.capture = capture # callback function for capturing and publishing images def image_callback(self): # check if the video capture is open if self.capture.isopened(): # read a frame from the video capture ret, frame = self.capture.read() # resize the frame to 640x480 frame = cv2.resize(frame, (640, 480)) # create an image message msg = image() # set the message header timestamp and frame id msg.header.stamp = node.get_clock(self).now().to_msg() msg.header.frame_id = "base" # set the width and height of the image msg.width = frame.shape[1] msg.height = frame.shape[0] # set the encoding format of the image msg.encoding = "bgr8" # specify the endianness of the image data msg.is_bigendian = false # set the step size (row length in bytes) msg.step = np.shape(frame)[2] * np.shape(frame)[1] # convert the frame to bytes and set the image data msg.data = np.array(frame).tobytes() # publish the image message self.publisher_.publish(msg) # log and print the number of images published self.get_logger().info('%d images published' % self.i) # increment the image counter self.i += 1 # main function to initialize and run the node def main(args=none): # get the video path from command-line arguments video_path = sys.argv[1] # open a connection to the specified video file capture = cv2.videocapture(video_path) # set the buffer size for the video capture capture.set(cv2.cap_prop_buffersize, 2) # initialize the rclpy library rclpy.init(args=args) # initialize the camimagepublisher object cam_img_publisher = none # check if the video capture is opened successfully if not capture.isopened(): # print an error message if the video capture cannot be opened print("error: could not open video stream from webcam.") # destroy the node and shutdown rclpy cam_img_publisher.destroy_node() rclpy.shutdown() # release the video capture object capture.release() else: # create the camimagepublisher object and start the node cam_img_publisher = vodimagepublisher(capture) # spin the node to keep it running rclpy.spin(cam_img_publisher) if __name__ == '__main__': # call the main function main() lets break it down. main() : in the main() function, we first take the video path from the command line argument. read the video file using cv2.videocapture() and initialize the rclpy, rclpy is a python client library for ros 2, which allows you to create nodes, publish and subscribe to topics etc. after this vodimagepublisher is called which starts the node, and rclpy.spin(cam_img_publisher) keeps the node running until there is any outside intervention. vodimagepublisher : in the class initialization, self.create_publisher is used to create a publisher, which takes the message type, topic name, queue size. self.create_timer is used to execute the callback function every timer_period . in the image_callback function we first check if the video frames are coming using self.capture.isopened() , if yes, then read the frame using self.capture.read() . now these frames are converted into image message type. for doing that we first create an object of image() class and fill the header , mentioning frame_id and stamp is very important, after that we fill the other fields which are width, height, encoding and data. after that we publish the message using self.publisher_.publish(msg) . # create an image message msg = image() msg.header.stamp = node.get_clock(self).now().to_msg() msg.header.frame_id = "base" msg.width = frame.shape[1] msg.height = frame.shape[0] msg.encoding = "bgr8" msg.is_bigendian = false msg.step = np.shape(frame)[2] * np.shape(frame)[1] msg.data = np.array(frame).tobytes() some people have difficulty understanding, what is queue size ? queue size is nothing but the buffer size. a publisher having queue size 10, means that if the publishing rate exceeds the rate at which messages can be sent then the buffer temporarily stores these messages. if the buffer reaches its capacity of 10 messages, the oldest messages will be discarded to make room for new ones, ensuring that the most recent messages are prioritized for delivery. now that we have figured out how to get the video frame from a topic, lets write the slam_pub.py to get the slam output. after that we will set up the configuration files to run the nodes. slam ros 2 publisher subscriber loop in python: in this slam_pub.py file we will have both publisher and subscriber. publishers: map point publisher: publishes the 3d points in /cam/map_points topic. camera pose publisher: publishes camera pose in /cam/camera_pose topic. feature points: publishes the feature points in /cam/feature_ext_viz topic. subscriber: we subscribed to the /cam/image_raw topic as the input to the slam algorithm. ros 2 monocular slam code we start by importing the necessary libraries. we import the slam code as, from slam.pyslam import process_frame , here process_frame is the function that takes the image frames as input and produces the map point and camera pose as output. we import it from slam.pyslam slam is the folder name and pyslam is the file name. import rclpy import numpy as np from rclpy.node import node from sensor_msgs.msg import image from cv_bridge import cvbridge, cvbridgeerror from visualization_msgs.msg import marker, markerarray from slam.pyslam import process_frame # assuming process_frame is a function from the slam package from sensor_msgs.msg import pointcloud2, pointfield class slampublisher(node): def __init__(self): super().__init__('slam_pub') # publisher for point cloud data with a larger queue size to handle high message rates self.pcd_publisher = self.create_publisher(pointcloud2, '/cam/map_points', 100) # publisher for markers representing camera poses with a moderate queue size self.marker_publisher = self.create_publisher(markerarray, '/cam/camera_pose', 10) # publisher for visualizing feature extraction results with a moderate queue size self.feature_publisher = self.create_publisher(image, '/cam/feature_ext_viz', 10) # subscriber for raw camera images with a moderate queue size self.subscription = self.create_subscription( image, '/cam/image_raw', self.listener_callback, 10) self.subscription # prevent unused variable warning self.bridge = cvbridge() # bridge to convert between ros and opencv images self.timer_period = 0.5 # timer period in seconds self.timer = self.create_timer(self.timer_period, self.publish_callback) # create a timer to call the publish callback periodically self.latest_frame = none # variable to store the latest frame self.itr = 0 # iterator counter def listener_callback(self, msg): # callback function to handle incoming images self.latest_frame = self.bridge.imgmsg_to_cv2(msg, "bgr8") def publish_callback(self): if self.latest_frame is not none: frame = self.latest_frame result = process_frame(frame) # process the frame using the slam library if result is not none: img, mapp = result # unpack the result from process_frame marker_array = markerarray() # create markers for each frame in the map for i, frame in enumerate(mapp.frames): pose = frame.pose translation = pose[0:3, 3] marker = marker() marker.header.frame_id = "base" marker.header.stamp = self.get_clock().now().to_msg() marker.ns = "slam_points" marker.id = i marker.type = marker.sphere # use a cube to represent the marker marker.action = marker.add marker.pose.position.x = translation[0] marker.pose.position.y = translation[1] marker.pose.position.z = translation[2] marker.pose.orientation.x = 0.0 marker.pose.orientation.y = 0.0 marker.pose.orientation.z = 0.0 marker.pose.orientation.w = 1.0 marker.scale.x = 0.3 marker.scale.y = 0.3 marker.scale.z = 0.4 marker.color.a = 1.0 marker.color.r = 1.0 marker.color.g = 0.0 marker.color.b = 0.0 marker_array.markers.append(marker) self.marker_publisher.publish(marker_array) # publish the marker array self.get_logger().info('publishing markers') # create a pointcloud2 message msg = pointcloud2() msg.header.stamp = self.get_clock().now().to_msg() msg.header.frame_id = 'base' points = np.array(mapp.points) msg.height = 1 msg.width = points.shape[0] # define the fields for the point cloud msg.fields = [ pointfield(name='x', offset=0, datatype=pointfield.float32, count=1), pointfield(name='y', offset=4, datatype=pointfield.float32, count=1), pointfield(name='z', offset=8, datatype=pointfield.float32, count=1) ] msg.is_bigendian = false msg.point_step = 12 msg.row_step = msg.point_step * points.shape[0] msg.is_dense = true msg.data = np.asarray(points, np.float32).tobytes() self.pcd_publisher.publish(msg) # publish the point cloud self.get_logger().info('publishing point cloud') # convert the image to a ros image message and publish it image_message = self.bridge.cv2_to_imgmsg(img, encoding="bgr8") self.feature_publisher.publish(image_message) self.get_logger().info('published feature extraction image') else: self.get_logger().warn("function returned none, cannot unpack") def main(args=none): rclpy.init(args=args) # initialize the rclpy library minimal_subscriber = slampublisher() # create an instance of the slampublisher node rclpy.spin(minimal_subscriber) # spin the node to keep it running # destroy the node explicitly (optional - otherwise it will be done automatically) minimal_subscriber.destroy_node() rclpy.shutdown() # shutdown the node if __name__ == '__main__': main() as before we start from main() function, rclpy.init(args=args) is how we start the ros2 client. minimal_subscriber = slampublisher() we create an object of slampublisher class. rclpy.spin(minimal_subscriber) keeps the node running. the code below that line is not executed until there is any external intervention. if any external intervention is applied then the node is destroyed using the below code, minimal_subscriber.destroy_node() rclpy.shutdown() # shutdown the node now lets look how slampublisher is working. we first define the three publishers . we publish point cloud data in the /cam/map_points topic, in the /cam/camera_pose we are publishing the sequence of markers, here its a sphere and finally we publish the feature extraction image in the /cam/feature_ext_viz topic. the queue size for the point cloud publisher is kept 100, because the map point processing was quite fast. # publisher for point cloud data with self.pcd_publisher = self.create_publisher(pointcloud2, '/cam/map_points', 100) # publisher for markers representing camera poses self.marker_publisher = self.create_publisher(markerarray, '/cam/camera_pose', 10) # publisher for visualizing feature extraction results self.feature_publisher = self.create_publisher(image, '/cam/feature_ext_viz', 10) after that we define the video frame subscriber using self.create_subscription . this function takes message type (image), topic name ( /cam/image_raw ), callback function ( self.listener_callback ) and queue size( 10 ). as the object self.subscription is not called again in the code we write self.subscription in the next line, otherwise it will throw a warning. in the 29th line we call the publisher callback . in this single callback we are publishing point cloud, camera pose and feature extraction image. in the listener_callback function we get the image frame and update the self.latest_frame variable with the latest image frames. this variable is used as input in the publisher callback publish_callback() . we first check if self.latest_frame is not none, if thats true we pass the image to the process_frame() function. after that we get the mapp and img from the result of that function. mapp object has two attributes, .points which contains the 3d map points and .frames which contains the camera pose . mapp.frames is a list containing the camera pose, and with each input it appends the new pose to the list. as it is a sequence of poses, we use markerarray to visualize the point in rviz2 . markerarray is an array of markers, each marker has a type, here it is a sphere. marker.scale decides the size of the sphere, marker.color defines the color, marker.pose.position and marker.pose.orientation need to be filled up with the camera position and orientation . mapp.points is also a list, and appends new points with each iteration. we convert the map points into point-cloud messages. the feature extraction images are published using cvbridge . now that we are done with coding, lets update the configuration files. we will look into the following files, launch/slam.launch.py : a single file for executing all the nodes consecutively. setup.py : we will update the external files we will use to test the slam code. also provide the launch file path, so that the package can understand that the launch file actually belongs to that package itself. creating a launch file in python: writing a launch file in python is very simple, first we define an unique function generate_launch_description() under which we mention which nodes to execute and in which order. the launch file name needs to end with launch.py to be recognized and autocomplete by ros2 launch . your launch file should define the generate_launch_description() function which returns a launch.launchdescription() to be used by the ros2 launch verb. first we need to import two important objects, launchdescription and node . from launch import launchdescription from launch_ros.actions import node def generate_launch_description(): ld = launchdescription() video_pub = node( package="slam", executable="vod_img_pub", # ensure this matches the entry point defined in your setup.py or cmakelists.txt name='vod_img_pub', # optional, but useful for debugging arguments=['src/slam/videos/test_ohio.mp4'], output='screen', emulate_tty=true, # ensures proper handling of console output ) slam_pub = node( package='slam', executable='slam_pub', name='slam_pub', output='screen', emulate_tty=true, # optional, useful for debugging ) rviz = node( package="rviz2", executable="rviz2", # ensure this matches the entry point defined in your setup.py or cmakelists.txt output='screen', arguments=['-d' + 'rviz/py_slam_ros2.rviz'] ) ld.add_action(video_pub) ld.add_action(slam_pub) ld.add_action(rviz) return ld after the imports we create an instance of launchdescription() and later we will add the nodes in that. now, we will create the nodes. as discussed before there will be one node for publishing the video frames and another for slam output . we will also create a node for rviz2, so that we can visualize the topics. we will discuss how to define one node , which will be enough to understand how to add python package nodes to launch files . video_pub = node( package="slam", executable="vod_img_pub", name='vod_img_pub', arguments=['src/slam/videos/test_ohio.mp4'], output='screen', emulate_tty=true, ) in the above code, we initiate the node class, with package name, which is slam in our case, name of the node vod_img_pub , name field is generally optional, you can rename the node using this. the command line argument is passed using arguments parameter. output='screen' means that the output will be shown in the terminal screen. you can provide log as input, by doing that the output will directly go to the log files not in the screen. if you provide prefix='xterm -e' and keep output='screen' then output will be shown in a different xterm window. emulate_tty=true is provided for debugging. after defining the nodes we add that to the launch description object like this, ld.add_action(video_pub) . and finally return ld . update setup.py : there are only two changes in the setup.py , update the launch file location in the data_files argument. update the entry_points , which is nothing but a description of from which function the execution should start, its formatted as command_name = package_name.file_name:function_name . from setuptools import find_packages, setup from glob import glob import os package_name = 'slam' setup( name=package_name, version='0.0.0', packages=find_packages(exclude=['test']), data_files=[ ('share/ament_index/resource_index/packages', ['resource/' + package_name]), ('share/' + package_name, ['package.xml']), (os.path.join('share', package_name), glob('launch/*.launch.py')) ], install_requires=['setuptools'], zip_safe=true, maintainer='somusan', maintainer_email='your@gmail.com', description='todo: package description', license='todo: license declaration', tests_require=['pytest'], entry_points={ 'console_scripts': [ "vod_img_pub = slam.vod_pub:main", "slam_pub = slam.slam_pub:main" ], }, ) now that we are done with all the code and configuration setup, lets build the package and run it. before doing that make sure you have installed required python packages using the command pip install -r requirements.txt . we will build the package with the below command,  cd ../../ # come to the workspace home directory  colcon build # this will build all the packages inside src  source install/setup.bash # source the workspace  ros2 launch slam slam.launch.py ros 2 important commands further more you can use various ros 2 command to see which topics are available, which nodes are running, at what rate the messages are being published.  ros2 topic list : shows the available topics.  ros2 node list : shows currently running nodes.  ros2 topic hz /example_topic : gives the rate of message published in that topic  ros2 topic show /example_topic : shows the message thats being published in that topic.  ros2 node info /node1 : get information  a specific node  ros2 bag record /topic1 /topic2 /topic3 : record a bagfile from the specified topics  ros2 bag record -a : record all available topics  ros2 bag record -o my_bag /topic_name : by default, the bag file will be saved in a directory named after the current timestamp. you can specify a custom directory name using the -o option  ros2 bag play my_bag : to play back a recorded bag file, use the ros 2 bag play command.  ros2 bag info my_bag : to list information  a bag file, such as the topics it contains and the message types: key takeaways ros 2 has evolved significantly from its initial version (ros1), addressing critical needs such as security, reliability, and support for complex, large-scale embedded systems. this evolution has made ros 2 an essential tool for modern robotics development, used widely by individuals and companies globally. the integration of data distribution service (dds) in ros 2 provides a robust communication framework, facilitating efficient, scalable, and real-time data exchange. dds supports advanced quality of service (qos) policies, which are crucial for ensuring data reliability, low latency, and efficient resource usage in mission-critical applications. essential ros 2 components include nodes, topic publishers, subscribers, services, and clients. nodes, which can be either standard or lifecycle managed, offer enhanced modularity, flexibility, and performance. topic publishers and subscribers enable communication across different parts of the system, while services and clients facilitate synchronous, request-response interactions for tasks that require confirmation or specific results. the article provides a comprehensive guide on integrating monocular slam with ros 2 using python. it covers the creation of ros 2 workspaces, packages, and nodes, demonstrating how to publish and subscribe to topics, and implement services and clients. this practical example underscores the capability of ros 2 to handle complex robotics tasks, from hardware integration to building autonomous systems. conclusion ros is a very common component in robotics and has many technical tutorials and resources available on the internet. however, through this , our objective is to provide a detailed understanding of the internal workings of ros 2, how dds works , the need for dds, the ros1 middleware architecture , and the data flow in ros 2 . additionally, we discuss how to use this tool in python, covering various topics such as packages, nodes, topics, publishers, subscribers, and services. at the end, for more hands-on understanding, we have created a capstone project where we i ntegrate monocular slam with ros 2 using python . we hope this will serve as a beginner-friendly gateway for anyone who wants to learn ros 2 and get into robotics. please follow the robotics  series for more such articles. references ros 2 + dds interoperation ros 2 humble documentation robot operating system 2: design, architecture, and uses in the wild load comments this course is available for free only till 22 nd nov. free python course we have designed this python course in collaboration with opencv.org for you to build a strong foundation in the essential elements of python, jupyter, numpy and matplotlib. name email get started we hate spam and promise to keep your email address safe. free opencv crash course we have designed this free crash course in collaboration with opencv.org to help you take your first steps into the fascinating world of artificial intelligence and computer vision. the course will be delivered straight into your mailbox. name email get started we hate spam and promise to keep your email address safe. get started with opencv free opencv crash course getting started guides installation packages c++ and python examples newsletter insights name email get started we hate spam and promise to keep your email address safe. subscribe to receive the download link, receive updates, and be notified of bug fixes which email should i send you the download link? first name email post title code url download the code we hate spam and promise to keep your email address safe. subscribe to receive free opencv crash course getting started guides installation packages c++ and python examples newsletter insights we hate spam and promise to keep your email address safe. name email get started we hate spam and promise to keep your email address safe. subscribe now submit  learnopencv empowering innovation through education, learnopencv provides in-depth tutorials, code, and guides in ai, computer vision, and deep learning. led by dr. satya mallick, we're dedicated to nurturing a community keen on technology breakthroughs. read more free courses categories getting started opencv university   2024  big vision llc  policy  and conditions